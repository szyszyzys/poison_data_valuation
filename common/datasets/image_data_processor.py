#!/usr/bin/env python3
"""
federated_data_setup.py

A robust and modular script to prepare federated datasets for data marketplace simulations.
This module supports various data partitioning strategies and facilitates the simulation
of data-based attacks like property inference by controlling attribute prevalence.
"""
import json
import logging
import os
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
from PIL import Image
from torch.utils.data import Dataset
from torchvision import datasets, transforms

# --- Setup logging for better feedback ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'
)
logger = logging.getLogger(__name__)


# --- Data Class for Attack Configuration ---
@dataclass
class AttackConfig:
    """A structured way to define a data-based attack."""
    attack_type: str
    malicious_client_ids: List[int]
    attack_params: Dict[str, Any]


# --- 1. Custom Datasets & Loading ---

class CelebACustom(datasets.CelebA):
    """Wrapper for CelebA to handle partitioning by any specified attribute."""

    def __init__(self, root: str, split: str, transform: Optional[callable] = None,
                 download: bool = True, property_key: str = 'Blond_Hair'):
        super().__init__(root=root, split=split, target_type=["identity", "attr"],
                         transform=transform, download=download)

        self.property_key = property_key.lower()
        try:
            # Find the index for the specified attribute
            self.property_idx = self.attr_names.index(property_key)
            logger.info(f"'{property_key}' attribute found at index {self.property_idx}.")
        except ValueError:
            raise RuntimeError(f"Could not find '{property_key}' in CelebA attributes.")

    def has_property(self, item_idx: int) -> bool:
        """Checks if a sample has the property specified during initialization."""
        # self.attr is a tensor of [N, 40]
        return self.attr[item_idx][self.property_idx].item() == 1


class Camelyon16Custom(Dataset):
    """
    Custom Dataset for Camelyon16, exposing metadata for partitioning by hospital or tumor prevalence.
    """

    def __init__(self, root: str, transform: Optional[callable] = None, download: bool = True):
        self.root = root
        self.transform = transform
        self.data_dir = os.path.join(self.root, 'camelyon16_patches')
        # This metadata file will be generated by the dummy download function
        self.metadata_path = os.path.join(self.data_dir, 'patch_meta.csv')
        if download and not os.path.exists(self.metadata_path):
            self._download_and_extract()
        self.metadata = pd.read_csv(self.metadata_path)
        self.image_paths = [os.path.join(self.data_dir, 'patches', f) for f in self.metadata['filename']]
        # The main label for classification
        self.targets = self.metadata['label'].values
        logger.info(f"Camelyon16 loaded. Found {len(self.image_paths)} patches.")

    def __len__(self) -> int:
        return len(self.image_paths)

    def __getitem__(self, idx: int) -> Tuple[Any, int]:
        image = Image.open(self.image_paths[idx]).convert("RGB")
        label = int(self.targets[idx])
        if self.transform:
            image = self.transform(image)
        return image, label

    def has_property(self, item_idx: int, property_key: str) -> bool:
        """Checks if a sample has a given property based on metadata."""
        property_key = property_key.lower()

        if property_key == 'tumor':
            # 'label' column: 1 for tumor, 0 for normal
            return self.metadata.iloc[item_idx]['label'] == 1
        elif property_key in self.metadata.columns:
            return pd.notna(self.metadata.iloc[item_idx][property_key])

        return False

    def _download_and_extract(self):
        """Creates a DUMMY Camelyon16 dataset for demonstration purposes."""
        patches_dir = os.path.join(self.data_dir, 'patches')
        os.makedirs(patches_dir, exist_ok=True)
        logger.warning("Creating a DUMMY Camelyon16 dataset.")

        metadata_list = []
        # UMC Utrecht (Buyer/Test set)
        for i in range(400):
            # Standard prevalence ~15%
            label = 1 if np.random.rand() < 0.15 else 0
            fname = f'dummy_utrecht_{i}.png'
            color = 'magenta' if label == 1 else 'blue'
            Image.new('RGB', (96, 96), color=color).save(os.path.join(patches_dir, fname))
            metadata_list.append({'filename': fname, 'label': label, 'center': 'Utrecht'})
        # Radboud UMC (Seller set)
        for i in range(1600):
            label = 1 if np.random.rand() < 0.15 else 0
            fname = f'dummy_radboud_{i}.png'
            color = 'red' if label == 1 else 'grey'
            Image.new('RGB', (96, 96), color=color).save(os.path.join(patches_dir, fname))
            metadata_list.append({'filename': fname, 'label': label, 'center': 'Radboud'})

        dummy_metadata_df = pd.DataFrame(metadata_list)
        dummy_metadata_df.to_csv(self.metadata_path, index=False)


def load_dataset(name: str, root: str = "./data", download: bool = True) -> Tuple[Dataset, Optional[Dataset]]:
    """Loads the specified training and testing datasets."""
    logger.info(f"Loading '{name}' dataset...")
    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
    if name.lower() == "camelyon16":
        return Camelyon16Custom(root, transform=transform, download=download), None
    elif name.lower() == "celeba":
        return CelebACustom(root, split='all', transform=transform, download=download), None
    # Add other datasets if needed
    raise NotImplementedError(f"Dataset '{name}' is not supported in this configuration.")


# --- 3. Statistics and Orchestration ---

def save_data_statistics(buyer_indices, seller_splits, client_properties, targets, output_dir) -> Dict:
    """Calculates, logs, and saves the data distribution statistics."""
    stats = {"buyer": {}, "sellers": {}, "client_properties": client_properties}
    # Buyer stats
    buyer_targets = targets[buyer_indices]
    stats["buyer"]["total_samples"] = len(buyer_indices)
    logger.info(f"Buyer Stats: {len(buyer_indices)} samples.")

    # Seller stats
    for cid, indices in seller_splits.items():
        stats["sellers"][cid] = {"total_samples": len(indices), "property": client_properties.get(cid, "N/A")}
        logger.info(f"Client {cid} ({stats['sellers'][cid]['property']}): {len(indices)} samples.")

    os.makedirs(output_dir, exist_ok=True)
    with open(os.path.join(output_dir, 'data_statistics.json'), 'w') as f:
        json.dump(stats, f, indent=4)
    logger.info(f"Data statistics saved to {os.path.join(output_dir, 'data_statistics.json')}")
    return stats


# You would also need a slight modification to load_dataset to handle the property_key for CelebA
def load_dataset_with_property(name: str, root: str = "./data", download: bool = True,
                               property_key: str = 'Blond_Hair') -> Tuple[Dataset, Optional[Dataset]]:
    """Loads datasets, allowing dynamic property selection for CelebA."""
    logger.info(f"Loading '{name}' dataset...")
    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
    if name.lower() == "camelyon16":
        return Camelyon16Custom(root, transform=transform, download=download), None
    elif name.lower() == "celeba":
        return CelebACustom(root, split='all', transform=transform, download=download, property_key=property_key), None
    raise NotImplementedError(f"Dataset '{name}' is not supported in this configuration.")
