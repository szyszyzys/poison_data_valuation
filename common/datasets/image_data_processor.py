#!/usr/bin/env python3
"""
federated_data_setup.py

A robust and modular script to prepare federated datasets for data marketplace simulations.
This module supports various data partitioning strategies and facilitates the simulation
of data-based attacks like property inference by controlling attribute prevalence.
"""
import collections
import json
import logging
import os
import re
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
import torch
from PIL import Image
from torch.utils.data import Dataset
from torchvision import datasets, transforms

# --- Setup logging for better feedback ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'
)
logger = logging.getLogger(__name__)


# --- Data Class for Attack Configuration ---
@dataclass
class AttackConfig:
    """A structured way to define a data-based attack."""
    attack_type: str
    malicious_client_ids: List[int]
    attack_params: Dict[str, Any]


# --- 1. Custom Datasets & Loading ---

class CelebACustom(datasets.CelebA):
    """Wrapper for CelebA to handle partitioning by any specified attribute."""

    def __init__(self, root: str, split: str, transform: Optional[callable] = None,
                 download: bool = True, property_key: str = 'Blond_Hair'):
        # More efficient: Only request the 'attr' target type.
        super().__init__(root=root, split=split, target_type="attr",
                         transform=transform, download=download)

        self.property_key = property_key.lower()
        try:
            self.property_idx = self.attr_names.index(property_key)
        except ValueError:
            raise RuntimeError(f"Could not find '{property_key}' in CelebA attributes.")

    # has_property method is fine, no changes needed.
    def has_property(self, item_idx: int) -> bool:
        return self.attr[item_idx][self.property_idx].item() == 1

    def __getitem__(self, index: int):
        # Now the parent directly returns (img, attributes_tensor)
        img, attrs = super().__getitem__(index)

        # Select the single property you want for your label
        label = attrs[self.property_idx]

        # Ensure the label is a 0D tensor of the correct type
        label = torch.tensor(label, dtype=torch.long)

        return img, label


class Camelyon16Custom(Dataset):
    """
    Custom Dataset for Camelyon16, exposing metadata for partitioning by hospital or tumor prevalence.
    """

    def __init__(self, root: str, transform: Optional[callable] = None, download: bool = True):
        self.root = root
        self.transform = transform
        self.data_dir = os.path.join(self.root, 'camelyon16_patches')
        # This metadata file will be generated by the dummy download function
        self.metadata_path = os.path.join(self.data_dir, 'patch_meta.csv')
        if download and not os.path.exists(self.metadata_path):
            self._download_and_extract()
        self.metadata = pd.read_csv(self.metadata_path)
        self.image_paths = [os.path.join(self.data_dir, 'patches', f) for f in self.metadata['filename']]
        # The main label for classification
        self.targets = self.metadata['label'].values
        logger.info(f"Camelyon16 loaded. Found {len(self.image_paths)} patches.")

    def __len__(self) -> int:
        return len(self.image_paths)

    def __getitem__(self, idx: int) -> Tuple[Any, int]:
        image = Image.open(self.image_paths[idx]).convert("RGB")
        label = int(self.targets[idx])
        if self.transform:
            image = self.transform(image)
        return image, label

    def has_property(self, item_idx: int, property_key: str) -> bool:
        """Checks if a sample has a given property based on metadata."""
        property_key = property_key.lower()

        if property_key == 'tumor':
            # 'label' column: 1 for tumor, 0 for normal
            return self.metadata.iloc[item_idx]['label'] == 1
        elif property_key in self.metadata.columns:
            return pd.notna(self.metadata.iloc[item_idx][property_key])

        return False

    def _download_and_extract(self):
        """Creates a DUMMY Camelyon16 dataset for demonstration purposes."""
        patches_dir = os.path.join(self.data_dir, 'patches')
        os.makedirs(patches_dir, exist_ok=True)
        logger.warning("Creating a DUMMY Camelyon16 dataset.")

        metadata_list = []
        # UMC Utrecht (Buyer/Test set)
        for i in range(400):
            # Standard prevalence ~15%
            label = 1 if np.random.rand() < 0.15 else 0
            fname = f'dummy_utrecht_{i}.png'
            color = 'magenta' if label == 1 else 'blue'
            Image.new('RGB', (96, 96), color=color).save(os.path.join(patches_dir, fname))
            metadata_list.append({'filename': fname, 'label': label, 'center': 'Utrecht'})
        # Radboud UMC (Seller set)
        for i in range(1600):
            label = 1 if np.random.rand() < 0.15 else 0
            fname = f'dummy_radboud_{i}.png'
            color = 'red' if label == 1 else 'grey'
            Image.new('RGB', (96, 96), color=color).save(os.path.join(patches_dir, fname))
            metadata_list.append({'filename': fname, 'label': label, 'center': 'Radboud'})

        dummy_metadata_df = pd.DataFrame(metadata_list)
        dummy_metadata_df.to_csv(self.metadata_path, index=False)


def load_dataset(name: str, root: str = "./data", download: bool = True) -> Tuple[Dataset, Optional[Dataset]]:
    """Loads the specified training and testing datasets."""
    logger.info(f"Loading '{name}' dataset...")
    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
    if name.lower() == "camelyon16":
        return Camelyon16Custom(root, transform=transform, download=download), None
    elif name.lower() == "celeba":
        return CelebACustom(root, split='all', transform=transform, download=download), None
    # Add other datasets if needed
    raise NotImplementedError(f"Dataset '{name}' is not supported in this configuration.")


# --- 3. Statistics and Orchestration ---

def save_data_statistics(
    buyer_indices: np.ndarray,
    seller_splits: Dict[int, List[int]],
    test_indices: np.ndarray, # <-- ADDED ARGUMENT
    client_properties: Dict[int, str],
    targets: np.ndarray,
    save_filepath: str # Path object is also fine here
) -> Dict[str, Any]: # Changed return type hint
    """
    Calculates and saves detailed data distribution statistics
    (buyer, sellers, test set) to a specific file path.
    """
    stats = {
        "buyer": {
            "total_samples": len(buyer_indices),
            "label_distribution": {} # Initialize empty
        },
        # --- ADDED TEST SET STATS ---
        "test_set": {
            "total_samples": len(test_indices),
            "label_distribution": {} # Initialize empty
        },
        # --- END ADDITION ---
        "sellers": {},
        "client_properties": client_properties
    }

    # Calculate label distributions only if indices exist
    if len(buyer_indices) > 0:
        try:
             stats["buyer"]["label_distribution"] = dict(sorted(collections.Counter(targets[buyer_indices].tolist()).items()))
        except IndexError:
             logger.error("Error calculating buyer label distribution - indices might be out of bounds.")
             stats["buyer"]["label_distribution"] = {"error": "Index out of bounds"}


    # --- CALCULATE TEST SET DISTRIBUTION ---
    if len(test_indices) > 0:
        try:
             stats["test_set"]["label_distribution"] = dict(sorted(collections.Counter(targets[test_indices].tolist()).items()))
        except IndexError:
             logger.error("Error calculating test set label distribution - indices might be out of bounds.")
             stats["test_set"]["label_distribution"] = {"error": "Index out of bounds"}
    # --- END CALCULATION ---

    for cid, indices in seller_splits.items():
        if indices: # Check if indices list is not empty
            try:
                label_dist = dict(sorted(collections.Counter(targets[indices].tolist()).items()))
            except IndexError:
                 logger.error(f"Error calculating seller {cid} label distribution - indices might be out of bounds.")
                 label_dist = {"error": "Index out of bounds"}
            stats["sellers"][cid] = {
                "total_samples": len(indices),
                "property": client_properties.get(str(cid), client_properties.get(int(cid),"N/A")), # Handle string/int keys
                "label_distribution": label_dist
            }
        else:
             # Include entry even for empty clients
             stats["sellers"][cid] = {
                "total_samples": 0,
                "property": client_properties.get(str(cid), client_properties.get(int(cid),"N/A")),
                "label_distribution": {}
            }


    save_path = Path(save_filepath)
    save_path.parent.mkdir(parents=True, exist_ok=True)

    try:
        with open(save_path, 'w') as f:
            # Use NumPy encoder helper if targets might contain numpy types not handled by default json
            class NpEncoder(json.JSONEncoder):
                def default(self, obj):
                    if isinstance(obj, np.integer): return int(obj)
                    if isinstance(obj, np.floating): return float(obj)
                    if isinstance(obj, np.ndarray): return obj.tolist()
                    return super(NpEncoder, self).default(obj)
            json.dump(stats, f, indent=4, cls=NpEncoder) # Use the encoder
        logger.info(f"Data statistics saved to {save_path}")
    except Exception as e:
        logger.error(f"Failed to save data statistics to {save_path}: {e}")
        # Optionally re-raise or return partial stats

    return stats

class StandardVisionDataset(Dataset):
    """
    A wrapper for standard torchvision datasets to provide a unified
    .has_property() interface for property-skew partitioning.
    """

    def __init__(self, vision_dataset: Dataset):
        self.dataset = vision_dataset
        if not hasattr(self.dataset, 'targets'):
            raise TypeError("The provided dataset does not have a '.targets' attribute for labels.")
        self.targets = self.dataset.targets

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        return self.dataset[idx]

    def has_property(self, index: int, property_key: str) -> bool:
        """
        Checks if the sample at 'index' has a property defined by 'property_key'.
        Example property_key: "class_in_[0,1,2,3,4]"
        """
        if not property_key or not property_key.startswith("class_in_"):
            raise ValueError(f"Invalid property_key format for this dataset: '{property_key}'")

        match = re.search(r'\[(.*?)\]', property_key)
        if not match:
            raise ValueError(f"Could not parse class list from property_key: '{property_key}'")

        try:
            property_classes = set(map(int, match.group(1).split(',')))
        except (ValueError, IndexError):
            raise ValueError(f"Could not parse class list from property_key: '{property_key}'")

        sample_label = self.targets[index]
        return sample_label in property_classes


# You would also need a slight modification to load_dataset to handle the property_key for CelebA
def load_dataset_with_property(name: str, root: str = "./data", download: bool = True,
                               property_key: str = 'Blond_Hair') -> Tuple[Dataset, Optional[Dataset]]:
    """Loads datasets, allowing dynamic property selection for CelebA."""
    logger.info(f"Loading '{name}' dataset...")
    name_lower = name.lower()
    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
    if name_lower == "camelyon16":
        return Camelyon16Custom(root, transform=transform, download=download), None
    elif name_lower == "celeba":
        return CelebACustom(root, split='all', transform=transform, download=download, property_key=property_key), None
    elif name_lower == "fmnist":
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.5,), (0.5,))
        ])
        train_set = datasets.FashionMNIST(root=root, train=True, download=download, transform=transform)
        return StandardVisionDataset(train_set), None

    elif name_lower == "cifar10":
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                std=[0.229, 0.224, 0.225])
        ])

        train_set = datasets.CIFAR10(root=root, train=True, download=download, transform=transform)
        return StandardVisionDataset(train_set), None

    else:
        raise ValueError(f"Dataset '{name}' not recognized or supported.")
