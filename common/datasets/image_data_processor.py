#!/usr/bin/env python3
"""
federated_data_setup.py

A robust and modular script to prepare federated datasets for data marketplace simulations.
This module supports various data partitioning strategies and facilitates the simulation
of data-based attacks like property inference by controlling attribute prevalence.
"""
import json
import logging
import os
import random
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
from PIL import Image
from torch.utils.data import DataLoader, Dataset, Subset
from torchvision import datasets, transforms

from common.gradient_market_configs import PropertySkewConfig, AppConfig

# --- Setup logging for better feedback ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'
)
logger = logging.getLogger(__name__)


# --- Data Class for Attack Configuration ---
@dataclass
class AttackConfig:
    """A structured way to define a data-based attack."""
    attack_type: str
    malicious_client_ids: List[int]
    attack_params: Dict[str, Any]


# --- 1. Custom Datasets & Loading ---

class CelebACustom(datasets.CelebA):
    """Wrapper for CelebA to handle partitioning by any specified attribute."""

    def __init__(self, root: str, split: str, transform: Optional[callable] = None,
                 download: bool = True, property_key: str = 'Blond_Hair'):
        super().__init__(root=root, split=split, target_type=["identity", "attr"],
                         transform=transform, download=download)

        self.property_key = property_key.lower()
        try:
            # Find the index for the specified attribute
            self.property_idx = self.attr_names.index(property_key)
            logger.info(f"'{property_key}' attribute found at index {self.property_idx}.")
        except ValueError:
            raise RuntimeError(f"Could not find '{property_key}' in CelebA attributes.")

    def has_property(self, item_idx: int) -> bool:
        """Checks if a sample has the property specified during initialization."""
        # self.attr is a tensor of [N, 40]
        return self.attr[item_idx][self.property_idx].item() == 1


class Camelyon16Custom(Dataset):
    """
    Custom Dataset for Camelyon16, exposing metadata for partitioning by hospital or tumor prevalence.
    """

    def __init__(self, root: str, transform: Optional[callable] = None, download: bool = True):
        self.root = root
        self.transform = transform
        self.data_dir = os.path.join(self.root, 'camelyon16_patches')
        # This metadata file will be generated by the dummy download function
        self.metadata_path = os.path.join(self.data_dir, 'patch_meta.csv')
        if download and not os.path.exists(self.metadata_path):
            self._download_and_extract()
        self.metadata = pd.read_csv(self.metadata_path)
        self.image_paths = [os.path.join(self.data_dir, 'patches', f) for f in self.metadata['filename']]
        # The main label for classification
        self.targets = self.metadata['label'].values
        logger.info(f"Camelyon16 loaded. Found {len(self.image_paths)} patches.")

    def __len__(self) -> int:
        return len(self.image_paths)

    def __getitem__(self, idx: int) -> Tuple[Any, int]:
        image = Image.open(self.image_paths[idx]).convert("RGB")
        label = int(self.targets[idx])
        if self.transform:
            image = self.transform(image)
        return image, label

    def has_property(self, item_idx: int, property_key: str) -> bool:
        """Checks if a sample has a given property based on metadata."""
        property_key = property_key.lower()

        if property_key == 'tumor':
            # 'label' column: 1 for tumor, 0 for normal
            return self.metadata.iloc[item_idx]['label'] == 1
        elif property_key in self.metadata.columns:
            # ### NEW ###
            # This allows partitioning by any metadata column, e.g., 'center'.
            # This is more flexible but assumes binary or specific value checks.
            # You might need to adjust what value you're checking against.
            # For simplicity, let's assume it checks for presence (not None/NaN).
            return pd.notna(self.metadata.iloc[item_idx][property_key])

        return False

    def _download_and_extract(self):
        """Creates a DUMMY Camelyon16 dataset for demonstration purposes."""
        patches_dir = os.path.join(self.data_dir, 'patches')
        os.makedirs(patches_dir, exist_ok=True)
        logger.warning("Creating a DUMMY Camelyon16 dataset.")

        metadata_list = []
        # UMC Utrecht (Buyer/Test set)
        for i in range(400):
            # Standard prevalence ~15%
            label = 1 if np.random.rand() < 0.15 else 0
            fname = f'dummy_utrecht_{i}.png'
            color = 'magenta' if label == 1 else 'blue'
            Image.new('RGB', (96, 96), color=color).save(os.path.join(patches_dir, fname))
            metadata_list.append({'filename': fname, 'label': label, 'center': 'Utrecht'})
        # Radboud UMC (Seller set)
        for i in range(1600):
            label = 1 if np.random.rand() < 0.15 else 0
            fname = f'dummy_radboud_{i}.png'
            color = 'red' if label == 1 else 'grey'
            Image.new('RGB', (96, 96), color=color).save(os.path.join(patches_dir, fname))
            metadata_list.append({'filename': fname, 'label': label, 'center': 'Radboud'})

        dummy_metadata_df = pd.DataFrame(metadata_list)
        dummy_metadata_df.to_csv(self.metadata_path, index=False)


def load_dataset(name: str, root: str = "./data", download: bool = True) -> Tuple[Dataset, Optional[Dataset]]:
    """Loads the specified training and testing datasets."""
    logger.info(f"Loading '{name}' dataset...")
    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
    if name.lower() == "camelyon16":
        return Camelyon16Custom(root, transform=transform, download=download), None
    elif name.lower() == "celeba":
        return CelebACustom(root, split='all', transform=transform, download=download), None
    # Add other datasets if needed
    raise NotImplementedError(f"Dataset '{name}' is not supported in this configuration.")


def _extract_targets(dataset: Dataset) -> np.ndarray:
    if hasattr(dataset, "targets"):
        return np.array(dataset.targets)
    return np.array([dataset[i][1] for i in range(len(dataset))])


class FederatedDataPartitioner:
    """Handles the partitioning of a dataset for a federated learning scenario."""

    def __init__(self, dataset: Dataset, num_clients: int, seed: int = 42):
        self.dataset = dataset
        self.num_clients = num_clients
        self.seed = seed
        self.targets = _extract_targets(dataset)
        self.buyer_indices: np.ndarray = np.array([], dtype=int)
        self.client_indices: Dict[int, List[int]] = {i: [] for i in range(num_clients)}
        self.client_properties: Dict[int, str] = {}  # Ground truth for PIA
        random.seed(seed)
        np.random.seed(seed)

    def partition(self, strategy: str, buyer_config: Dict, partition_params: Dict):
        """Main partitioning dispatcher."""
        all_indices = np.arange(len(self.dataset))
        seller_pool_indices = all_indices
        self.test_indices = np.array([], dtype=int)  # Initialize test_indices

        # --- Stage 1: Define Buyer and Seller Pools Based on Metadata ---
        if isinstance(self.dataset, Camelyon16Custom):
            logger.info("Partitioning Camelyon16 based on 'center' metadata.")
            meta = self.dataset.metadata
            buyer_pool = all_indices[meta['center'] == 'Utrecht']
            seller_pool_indices = all_indices[meta['center'] == 'Radboud']

            # Split the buyer pool into a root set and a test set
            np.random.shuffle(buyer_pool)
            split_idx = int(len(buyer_pool) * buyer_config.get("root_set_fraction", 0.2))
            self.buyer_indices = buyer_pool[:split_idx]
            self.test_indices = buyer_pool[split_idx:]

        elif isinstance(self.dataset, CelebACustom):
            logger.info("Partitioning CelebA based on 'identity' metadata.")
            ids = self.dataset.identity.squeeze().numpy()
            buyer_ids = set(range(1, 101))  # Example: first 100 identities are buyers

            buyer_pool = all_indices[np.isin(ids, list(buyer_ids))]
            seller_pool_indices = all_indices[~np.isin(ids, list(buyer_ids))]

            np.random.shuffle(buyer_pool)
            self.buyer_indices = buyer_pool[:buyer_config.get("num_root_samples", 1000)]

        logger.info(
            f"Partitioning {len(seller_pool_indices)} seller samples among {self.num_clients} clients using '{strategy}' strategy.")

        if strategy == 'property-skew':
            # Pass the config object directly
            self._partition_property_skew(seller_pool_indices, PropertySkewConfig(**partition_params))
        else:
            raise ValueError(f"Unknown partitioning strategy: {strategy}")
        return self

    def _partition_property_skew(self, seller_pool_indices: np.ndarray, config: PropertySkewConfig):
        """Partitions sellers based on the prevalence of a specific data property."""
        num_low_prevalence = self.num_clients - config.num_high_prevalence_clients - config.num_security_attackers

        # --- Define client groups ---
        client_ids = list(range(self.num_clients))
        np.random.shuffle(client_ids)

        high_clients = client_ids[:config.num_high_prevalence_clients]
        low_clients = client_ids[
                      config.num_high_prevalence_clients: config.num_high_prevalence_clients + num_low_prevalence]
        security_clients = client_ids[config.num_high_prevalence_clients + num_low_prevalence:]

        for cid in high_clients: self.client_properties[cid] = f"High-Prevalence ({config.property_key})"
        for cid in low_clients: self.client_properties[cid] = f"Low-Prevalence ({config.property_key})"
        for cid in security_clients: self.client_properties[cid] = "Security-Attacker (Standard-Prevalence)"

        # --- Separate seller data by property ---
        prop_true_indices, prop_false_indices = [], []
        for idx in seller_pool_indices:
            # Note: The has_property method in the dataset now needs to know the property_key
            if self.dataset.has_property(idx, property_key=config.property_key):
                prop_true_indices.append(idx)
            else:
                prop_false_indices.append(idx)

        np.random.shuffle(prop_true_indices)
        np.random.shuffle(prop_false_indices)

        # --- Distribute data using pointers for efficiency ---
        samples_per_client = len(seller_pool_indices) // self.num_clients
        true_ptr, false_ptr = 0, 0

        def assign_data(client_list, prevalence):
            nonlocal true_ptr, false_ptr
            for client_id in client_list:
                num_prop_true = int(samples_per_client * prevalence)
                num_prop_false = samples_per_client - num_prop_true

                # Assign samples with the property
                end_true = true_ptr + num_prop_true
                self.client_indices[client_id].extend(prop_true_indices[true_ptr:end_true])
                true_ptr = end_true

                # Assign samples without the property
                end_false = false_ptr + num_prop_false
                self.client_indices[client_id].extend(prop_false_indices[false_ptr:end_false])
                false_ptr = end_false

        assign_data(high_clients, config.high_prevalence_ratio)
        assign_data(low_clients, config.low_prevalence_ratio)
        assign_data(security_clients, config.standard_prevalence_ratio)

    def get_splits(self) -> Tuple[np.ndarray, Dict[int, List[int]], np.ndarray]:
        """Returns the final buyer, client, and test index splits."""
        return self.buyer_indices, self.client_indices, getattr(self, 'test_indices', np.array([]))


# --- 3. Statistics and Orchestration ---

def save_data_statistics(buyer_indices, seller_splits, client_properties, targets, output_dir) -> Dict:
    """Calculates, logs, and saves the data distribution statistics."""
    stats = {"buyer": {}, "sellers": {}, "client_properties": client_properties}
    # Buyer stats
    buyer_targets = targets[buyer_indices]
    stats["buyer"]["total_samples"] = len(buyer_indices)
    logger.info(f"Buyer Stats: {len(buyer_indices)} samples.")

    # Seller stats
    for cid, indices in seller_splits.items():
        stats["sellers"][cid] = {"total_samples": len(indices), "property": client_properties.get(cid, "N/A")}
        logger.info(f"Client {cid} ({stats['sellers'][cid]['property']}): {len(indices)} samples.")

    os.makedirs(output_dir, exist_ok=True)
    with open(os.path.join(output_dir, 'data_statistics.json'), 'w') as f:
        json.dump(stats, f, indent=4)
    logger.info(f"Data statistics saved to {os.path.join(output_dir, 'data_statistics.json')}")
    return stats


# Updated to accept the AppConfig object
def setup_federated_marketplace(cfg: AppConfig, save_dir: str, seed: int = 42):
    """Main orchestration function to setup the entire federated data marketplace."""
    # Access config values from the dataclass
    train_set, _ = load_dataset(cfg.experiment.dataset_name)

    partitioner = FederatedDataPartitioner(train_set, cfg.experiment.n_sellers, seed)

    # Assumes your config YAML has a "data_partition" section
    partitioner.partition(
        strategy=cfg.data_partition.strategy,
        buyer_config=cfg.data_partition.buyer_config,
        partition_params=cfg.data_partition.partition_params
    )
    buyer_indices, seller_splits, test_indices = partitioner.get_splits()

    stats = save_data_statistics(
        buyer_indices, seller_splits, partitioner.client_properties,
        _extract_targets(train_set), save_dir
    )
    batch_size = cfg.training.batch_size
    buyer_loader = DataLoader(Subset(train_set, buyer_indices), batch_size=batch_size, shuffle=True)
    seller_loaders = {cid: DataLoader(Subset(train_set, indices), batch_size=batch_size, shuffle=True)
                      for cid, indices in seller_splits.items() if indices}
    test_loader = DataLoader(Subset(train_set, test_indices), batch_size=batch_size, shuffle=False)

    logger.info("Federated marketplace setup complete. DataLoaders are ready. ✅")
    return buyer_loader, seller_loaders, test_loader, stats


# Place this function within your federated_data_setup.py file

def get_image_dataset(cfg: AppConfig) -> Tuple[DataLoader, Dict[int, DataLoader], DataLoader, Dict]:
    """
    A unified function to load, partition, and prepare federated image datasets.

    This function orchestrates the entire data setup process based on the provided
    application configuration. It handles dataset loading, complex partitioning
    strategies (like property skew), and the creation of PyTorch DataLoaders.

    Args:
        cfg (AppConfig): The main application configuration object containing all
                         parameters for the experiment, data, and partitioning.

    Returns:
        A tuple containing:
        - buyer_loader (DataLoader): DataLoader for the buyer's root dataset.
        - seller_loaders (Dict[int, DataLoader]): A dictionary mapping client IDs
          to their respective DataLoaders.
        - test_loader (DataLoader): DataLoader for the global test set.
        - stats (Dict): A dictionary with detailed statistics about the data distribution.
    """
    logger.info(f"--- Starting Federated Dataset Setup for '{cfg.experiment.dataset_name}' ---")

    # 1. Load the base dataset using the helper function
    # The property_key for CelebA is now passed dynamically from the config
    property_key = cfg.data_partition.partition_params.get('property_key', 'Blond_Hair')

    if cfg.experiment.dataset_name.lower() == 'celeba':
        # Pass the property key at initialization
        train_set, _ = load_dataset_with_property(cfg.experiment.dataset_name, property_key=property_key)
    else:
        train_set, _ = load_dataset(cfg.experiment.dataset_name)

    # 2. Initialize the partitioner
    partitioner = FederatedDataPartitioner(
        dataset=train_set,
        num_clients=cfg.experiment.n_sellers,
        seed=cfg.seed
    )

    # 3. Execute the partitioning strategy defined in the config
    logger.info(f"Applying '{cfg.data_partition.strategy}' partitioning strategy...")
    partitioner.partition(
        strategy=cfg.data_partition.strategy,
        buyer_config=cfg.data_partition.buyer_config,
        partition_params=cfg.data_partition.partition_params
    )
    buyer_indices, seller_splits, test_indices = partitioner.get_splits()

    # 4. Generate and save statistics about the data distribution
    stats = save_data_statistics(
        buyer_indices=buyer_indices,
        seller_splits=seller_splits,
        client_properties=partitioner.client_properties,
        targets=_extract_targets(train_set),
        output_dir=cfg.experiment.save_path
    )

    # 5. Create the final DataLoader objects
    batch_size = cfg.training.batch_size
    buyer_loader = DataLoader(Subset(train_set, buyer_indices), batch_size=batch_size, shuffle=True)

    seller_loaders = {
        cid: DataLoader(Subset(train_set, indices), batch_size=batch_size, shuffle=True)
        for cid, indices in seller_splits.items() if indices
    }

    # Ensure test_loader is not empty before creating
    test_loader = None
    if len(test_indices) > 0:
        test_loader = DataLoader(Subset(train_set, test_indices), batch_size=batch_size, shuffle=False)
        logger.info(f"Created test loader with {len(test_indices)} samples.")
    else:
        logger.warning("No test indices found; test_loader will be None.")

    logger.info("✅ Federated dataset setup complete. DataLoaders are ready.")
    return buyer_loader, seller_loaders, test_loader, stats


# You would also need a slight modification to load_dataset to handle the property_key for CelebA
def load_dataset_with_property(name: str, root: str = "./data", download: bool = True,
                               property_key: str = 'Blond_Hair') -> Tuple[Dataset, Optional[Dataset]]:
    """Loads datasets, allowing dynamic property selection for CelebA."""
    logger.info(f"Loading '{name}' dataset...")
    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
    if name.lower() == "camelyon16":
        return Camelyon16Custom(root, transform=transform, download=download), None
    elif name.lower() == "celeba":
        return CelebACustom(root, split='all', transform=transform, download=download, property_key=property_key), None
    raise NotImplementedError(f"Dataset '{name}' is not supported in this configuration.")
