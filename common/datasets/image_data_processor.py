#!/usr/bin/env python3
"""
federated_data_setup.py

A robust and modular script to prepare federated datasets for data marketplace simulations.
This module supports various data partitioning strategies and facilitates the simulation
of data-based attacks like property inference by controlling attribute prevalence.
"""
import json
import logging
import os
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple, re

import numpy as np
import pandas as pd
import torch
from PIL import Image
from torch.utils.data import Dataset
from torchvision import datasets, transforms

# --- Setup logging for better feedback ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'
)
logger = logging.getLogger(__name__)


# --- Data Class for Attack Configuration ---
@dataclass
class AttackConfig:
    """A structured way to define a data-based attack."""
    attack_type: str
    malicious_client_ids: List[int]
    attack_params: Dict[str, Any]


# --- 1. Custom Datasets & Loading ---

class CelebACustom(datasets.CelebA):
    """Wrapper for CelebA to handle partitioning by any specified attribute."""

    def __init__(self, root: str, split: str, transform: Optional[callable] = None,
                 download: bool = True, property_key: str = 'Blond_Hair'):
        # More efficient: Only request the 'attr' target type.
        super().__init__(root=root, split=split, target_type="attr",
                         transform=transform, download=download)

        self.property_key = property_key.lower()
        try:
            self.property_idx = self.attr_names.index(property_key)
        except ValueError:
            raise RuntimeError(f"Could not find '{property_key}' in CelebA attributes.")

    # has_property method is fine, no changes needed.
    def has_property(self, item_idx: int) -> bool:
        return self.attr[item_idx][self.property_idx].item() == 1

    def __getitem__(self, index: int):
        # Now the parent directly returns (img, attributes_tensor)
        img, attrs = super().__getitem__(index)

        # Select the single property you want for your label
        label = attrs[self.property_idx]

        # Ensure the label is a 0D tensor of the correct type
        label = torch.tensor(label, dtype=torch.long)

        return img, label


class Camelyon16Custom(Dataset):
    """
    Custom Dataset for Camelyon16, exposing metadata for partitioning by hospital or tumor prevalence.
    """

    def __init__(self, root: str, transform: Optional[callable] = None, download: bool = True):
        self.root = root
        self.transform = transform
        self.data_dir = os.path.join(self.root, 'camelyon16_patches')
        # This metadata file will be generated by the dummy download function
        self.metadata_path = os.path.join(self.data_dir, 'patch_meta.csv')
        if download and not os.path.exists(self.metadata_path):
            self._download_and_extract()
        self.metadata = pd.read_csv(self.metadata_path)
        self.image_paths = [os.path.join(self.data_dir, 'patches', f) for f in self.metadata['filename']]
        # The main label for classification
        self.targets = self.metadata['label'].values
        logger.info(f"Camelyon16 loaded. Found {len(self.image_paths)} patches.")

    def __len__(self) -> int:
        return len(self.image_paths)

    def __getitem__(self, idx: int) -> Tuple[Any, int]:
        image = Image.open(self.image_paths[idx]).convert("RGB")
        label = int(self.targets[idx])
        if self.transform:
            image = self.transform(image)
        return image, label

    def has_property(self, item_idx: int, property_key: str) -> bool:
        """Checks if a sample has a given property based on metadata."""
        property_key = property_key.lower()

        if property_key == 'tumor':
            # 'label' column: 1 for tumor, 0 for normal
            return self.metadata.iloc[item_idx]['label'] == 1
        elif property_key in self.metadata.columns:
            return pd.notna(self.metadata.iloc[item_idx][property_key])

        return False

    def _download_and_extract(self):
        """Creates a DUMMY Camelyon16 dataset for demonstration purposes."""
        patches_dir = os.path.join(self.data_dir, 'patches')
        os.makedirs(patches_dir, exist_ok=True)
        logger.warning("Creating a DUMMY Camelyon16 dataset.")

        metadata_list = []
        # UMC Utrecht (Buyer/Test set)
        for i in range(400):
            # Standard prevalence ~15%
            label = 1 if np.random.rand() < 0.15 else 0
            fname = f'dummy_utrecht_{i}.png'
            color = 'magenta' if label == 1 else 'blue'
            Image.new('RGB', (96, 96), color=color).save(os.path.join(patches_dir, fname))
            metadata_list.append({'filename': fname, 'label': label, 'center': 'Utrecht'})
        # Radboud UMC (Seller set)
        for i in range(1600):
            label = 1 if np.random.rand() < 0.15 else 0
            fname = f'dummy_radboud_{i}.png'
            color = 'red' if label == 1 else 'grey'
            Image.new('RGB', (96, 96), color=color).save(os.path.join(patches_dir, fname))
            metadata_list.append({'filename': fname, 'label': label, 'center': 'Radboud'})

        dummy_metadata_df = pd.DataFrame(metadata_list)
        dummy_metadata_df.to_csv(self.metadata_path, index=False)


def load_dataset(name: str, root: str = "./data", download: bool = True) -> Tuple[Dataset, Optional[Dataset]]:
    """Loads the specified training and testing datasets."""
    logger.info(f"Loading '{name}' dataset...")
    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
    if name.lower() == "camelyon16":
        return Camelyon16Custom(root, transform=transform, download=download), None
    elif name.lower() == "celeba":
        return CelebACustom(root, split='all', transform=transform, download=download), None
    # Add other datasets if needed
    raise NotImplementedError(f"Dataset '{name}' is not supported in this configuration.")


# --- 3. Statistics and Orchestration ---

def save_data_statistics(buyer_indices, seller_splits, client_properties, targets, output_dir) -> Dict:
    """Calculates, logs, and saves the data distribution statistics."""
    stats = {"buyer": {}, "sellers": {}, "client_properties": client_properties}
    # Buyer stats
    buyer_targets = targets[buyer_indices]
    stats["buyer"]["total_samples"] = len(buyer_indices)
    logger.info(f"Buyer Stats: {len(buyer_indices)} samples.")

    # Seller stats
    for cid, indices in seller_splits.items():
        stats["sellers"][cid] = {"total_samples": len(indices), "property": client_properties.get(cid, "N/A")}
        logger.info(f"Client {cid} ({stats['sellers'][cid]['property']}): {len(indices)} samples.")

    os.makedirs(output_dir, exist_ok=True)
    with open(os.path.join(output_dir, 'data_statistics.json'), 'w') as f:
        json.dump(stats, f, indent=4)
    logger.info(f"Data statistics saved to {os.path.join(output_dir, 'data_statistics.json')}")
    return stats


class StandardVisionDataset(Dataset):
    """
    A wrapper for standard torchvision datasets to provide a unified
    .has_property() interface for property-skew partitioning.
    """

    def __init__(self, vision_dataset: Dataset):
        self.dataset = vision_dataset
        if not hasattr(self.dataset, 'targets'):
            raise TypeError("The provided dataset does not have a '.targets' attribute for labels.")
        self.targets = self.dataset.targets

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        return self.dataset[idx]

    def has_property(self, index: int, property_key: str) -> bool:
        """
        Checks if the sample at 'index' has a property defined by 'property_key'.
        Example property_key: "class_in_[0,1,2,3,4]"
        """
        if not property_key or not property_key.startswith("class_in_"):
            raise ValueError(f"Invalid property_key format for this dataset: '{property_key}'")

        match = re.search(r'\[(.*?)\]', property_key)
        if not match:
            raise ValueError(f"Could not parse class list from property_key: '{property_key}'")

        try:
            property_classes = set(map(int, match.group(1).split(',')))
        except (ValueError, IndexError):
            raise ValueError(f"Could not parse class list from property_key: '{property_key}'")

        sample_label = self.targets[index]
        return sample_label in property_classes


# You would also need a slight modification to load_dataset to handle the property_key for CelebA
def load_dataset_with_property(name: str, root: str = "./data", download: bool = True,
                               property_key: str = 'Blond_Hair') -> Tuple[Dataset, Optional[Dataset]]:
    """Loads datasets, allowing dynamic property selection for CelebA."""
    logger.info(f"Loading '{name}' dataset...")
    name_lower = name.lower()
    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
    if name_lower == "camelyon16":
        return Camelyon16Custom(root, transform=transform, download=download), None
    elif name_lower == "celeba":
        return CelebACustom(root, split='all', transform=transform, download=download, property_key=property_key), None
    elif name_lower == "fmnist":
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.5,), (0.5,))
        ])
        train_set = datasets.FashionMNIST(root=root, train=True, download=download, transform=transform)
        return StandardVisionDataset(train_set), None

    elif name_lower == "cifar10":
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
        ])
        train_set = datasets.CIFAR10(root=root, train=True, download=download, transform=transform)
        return StandardVisionDataset(train_set), None

    else:
        raise ValueError(f"Dataset '{name}' not recognized or supported.")
