{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 1: Data Loading and Processing Functions\n",
    "\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def _parse_metadata_from_path(dir_name: str) -> Dict:\n",
    "    \"\"\"Helper to extract metadata like run number and seed from a directory name.\"\"\"\n",
    "    match = re.search(r\"run_(\\d+)_seed_(\\d+)\", dir_name)\n",
    "    if match:\n",
    "        return {\"run_id\": int(match.group(1)), \"seed\": int(match.group(2))}\n",
    "    return {\"run_id\": None, \"seed\": None}\n",
    "\n",
    "def _load_json_file(file_path: Path) -> Optional[Dict]:\n",
    "    \"\"\"Helper to safely load a JSON file from a given path.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        print(f\"Warning: Could not load or parse JSON file: {file_path}\")\n",
    "        return None\n",
    "\n",
    "def find_successful_runs(root_dir: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Scans a root directory for all successful experiment runs.\n",
    "\n",
    "    A run is considered successful if its directory contains a '.success' marker file.\n",
    "\n",
    "    Args:\n",
    "        root_dir: The path to the root directory containing experiment folders.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, where each dictionary represents a valid run\n",
    "        and contains its metadata and path.\n",
    "    \"\"\"\n",
    "    root_path = Path(root_dir)\n",
    "    if not root_path.is_dir():\n",
    "        raise FileNotFoundError(f\"Root directory not found: {root_path}\")\n",
    "\n",
    "    valid_runs = []\n",
    "    for run_dir in root_path.iterdir():\n",
    "        if not run_dir.is_dir():\n",
    "            continue\n",
    "\n",
    "        if (run_dir / \".success\").exists():\n",
    "            metadata = _parse_metadata_from_path(run_dir.name)\n",
    "            metadata['path'] = run_dir\n",
    "            valid_runs.append(metadata)\n",
    "\n",
    "    print(f\"Found {len(valid_runs)} successful experiment runs in {root_path}\")\n",
    "    return valid_runs\n",
    "\n",
    "def load_summary_df(runs: List[Dict]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregates final metrics and config parameters from multiple runs into a single DataFrame.\n",
    "\n",
    "    Args:\n",
    "        runs: A list of run dictionaries from find_successful_runs().\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame containing a summary of each run's final performance and config.\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "    for run in runs:\n",
    "        metrics = _load_json_file(run['path'] / \"final_metrics.json\")\n",
    "        config = _load_json_file(run['path'] / \"config_snapshot.json\")\n",
    "\n",
    "        if metrics and 'test_accuracy' in metrics and pd.notna(metrics['test_accuracy']):\n",
    "            summary = run.copy()\n",
    "            summary.update(metrics)\n",
    "\n",
    "            if config:\n",
    "                flat_config = pd.json_normalize(config, sep='_').to_dict(orient='records')[0]\n",
    "                summary.update(flat_config)\n",
    "\n",
    "            summaries.append(summary)\n",
    "\n",
    "    if not summaries:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(summaries)\n",
    "    for col in df.columns:\n",
    "        if df[col].apply(lambda x: isinstance(x, (list, dict))).any():\n",
    "            df[col] = df[col].astype(str)\n",
    "\n",
    "    return df.drop(columns=['path'])\n",
    "\n",
    "def load_learning_curves_df(runs: List[Dict]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads and concatenates the training_log.csv from multiple runs.\n",
    "\n",
    "    Args:\n",
    "        runs: A list of run dictionaries from find_successful_runs().\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame containing round-by-round data for plotting learning curves.\n",
    "    \"\"\"\n",
    "    all_logs = []\n",
    "    for run in runs:\n",
    "        log_file = run['path'] / \"training_log.csv\"\n",
    "        try:\n",
    "            log_df = pd.read_csv(log_file)\n",
    "            log_df['run_id'] = run['run_id']\n",
    "            log_df['seed'] = run['seed']\n",
    "            all_logs.append(log_df)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: training_log.csv not found for {run['path'].name}\")\n",
    "            continue\n",
    "\n",
    "    if not all_logs:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.concat(all_logs, ignore_index=True)\n",
    "\n",
    "def load_seller_analysis_df(runs: List[Dict]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads and concatenates the seller_round_metrics.csv from multiple runs.\n",
    "\n",
    "    Args:\n",
    "        runs: A list of run dictionaries from find_successful_runs().\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame for analyzing per-seller behavior over time.\n",
    "    \"\"\"\n",
    "    all_seller_logs = []\n",
    "    for run in runs:\n",
    "        seller_file = run['path'] / \"seller_round_metrics.csv\"\n",
    "        try:\n",
    "            seller_df = pd.read_csv(seller_file)\n",
    "            seller_df['run_id'] = run['run_id']\n",
    "            seller_df['seed'] = run['seed']\n",
    "            seller_df['seller_type'] = seller_df['seller_id'].apply(\n",
    "                lambda x: 'adversary' if 'adv' in x else 'benign'\n",
    "            )\n",
    "            all_seller_logs.append(seller_df)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: seller_round_metrics.csv not found for {run['path'].name}\")\n",
    "            continue\n",
    "\n",
    "    if not all_seller_logs:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.concat(all_seller_logs, ignore_index=True)\n",
    "\n",
    "# --- ADD THIS NEW FUNCTION TO CELL 1 ---\n",
    "\n",
    "def load_periodic_eval_df(runs: List[Dict]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads and concatenates the periodic evaluation results from the 'evaluations/'\n",
    "    subfolder of multiple runs.\n",
    "\n",
    "    Args:\n",
    "        runs: A list of run dictionaries from find_successful_runs().\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame containing round-by-round test set performance.\n",
    "    \"\"\"\n",
    "    all_evals = []\n",
    "    for run in runs:\n",
    "        eval_dir = run['path'] / \"evaluations\"\n",
    "        if not eval_dir.is_dir():\n",
    "            continue\n",
    "\n",
    "        for eval_file in eval_dir.glob(\"round_*.json\"):\n",
    "            match = re.search(r\"round_(\\d+).json\", eval_file.name)\n",
    "            if match:\n",
    "                round_num = int(match.group(1))\n",
    "                metrics = _load_json_file(eval_file)\n",
    "                if metrics:\n",
    "                    record = {\n",
    "                        'run_id': run['run_id'],\n",
    "                        'seed': run['seed'],\n",
    "                        'round': round_num,\n",
    "                        **metrics\n",
    "                    }\n",
    "                    all_evals.append(record)\n",
    "\n",
    "    if not all_evals:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.concat([pd.DataFrame([rec]) for rec in all_evals], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CELL 2: Visualization 1 - Accuracy vs. ASR\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.lines import Line2D\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 1. Configuration for Visualization ---\n",
    "# TODO: Adjust these parameters for your specific analysis.\n",
    "\n",
    "# The root directory containing your experiment runs.\n",
    "ROOT_EXPERIMENT_DIR = './path/to/your/experiment_outputs'\n",
    "\n",
    "# A name for the aggregation method you're analyzing (for titles and filenames).\n",
    "AGG_METHOD_TO_ANALYZE = \"FedAvg\"\n",
    "\n",
    "# Define the specific adversary rates you want to display on the plot's x-axis.\n",
    "ADV_RATES_TO_PLOT = [0.1, 0.2, 0.3, 0.4]\n",
    "\n",
    "# The directory where you want to save the generated plot.\n",
    "FIGURE_SAVE_DIR = Path(\"./analysis_plots\")\n",
    "\n",
    "# --- 2. Data Loading ---\n",
    "# This uses the functions defined in Cell 1.\n",
    "\n",
    "try:\n",
    "    all_runs = find_successful_runs(ROOT_EXPERIMENT_DIR)\n",
    "    # The summary_df now contains all config parameters as columns.\n",
    "    summary_df = load_summary_df(all_runs)\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    summary_df = pd.DataFrame() # Create empty df to avoid crashing the notebook\n",
    "\n",
    "# --- 3. Data Preprocessing for this Specific Plot ---\n",
    "\n",
    "if not summary_df.empty:\n",
    "    # Standardize column names for the plotting logic\n",
    "    summary_df = summary_df.rename(columns={\n",
    "        'test_accuracy': 'FINAL_MAIN_ACC',\n",
    "        'attack_success_rate': 'FINAL_ASR'\n",
    "    })\n",
    "\n",
    "    # Create the 'attack_method' and 'adv_rate' columns from the loaded config parameters\n",
    "    # This is much cleaner than parsing strings.\n",
    "    adv_rate_col = 'experiment_adv_rate' # This is the flattened key from your config\n",
    "    if adv_rate_col in summary_df.columns:\n",
    "        summary_df['adv_rate'] = summary_df[adv_rate_col]\n",
    "        summary_df['attack_method'] = np.where(summary_df[adv_rate_col] > 0, 'Backdoor', 'No Attack')\n",
    "    else:\n",
    "        print(f\"Warning: Column '{adv_rate_col}' not found. Using fallback values.\")\n",
    "        summary_df['adv_rate'] = 0.0\n",
    "        summary_df['attack_method'] = 'No Attack'\n",
    "\n",
    "# --- 4. Main Plotting Logic ---\n",
    "\n",
    "if not summary_df.empty and 'FINAL_MAIN_ACC' in summary_df.columns and 'FINAL_ASR' in summary_df.columns:\n",
    "\n",
    "    # --- Data Aggregation ---\n",
    "    no_attack_perf = summary_df[summary_df['attack_method'] == 'No Attack']\n",
    "    no_attack_avg_acc = no_attack_perf['FINAL_MAIN_ACC'].mean() if not no_attack_perf.empty else np.nan\n",
    "\n",
    "    backdoor_attack_filtered = summary_df[\n",
    "        (summary_df['attack_method'] == 'Backdoor') &\n",
    "        (summary_df['adv_rate'].isin(ADV_RATES_TO_PLOT))\n",
    "    ]\n",
    "\n",
    "    # Average results for each adversary rate across different seeds\n",
    "    backdoor_attack_avg_perf = backdoor_attack_filtered.groupby('adv_rate')[\n",
    "        ['FINAL_MAIN_ACC', 'FINAL_ASR']\n",
    "    ].mean().reset_index()\n",
    "\n",
    "    # --- Plotting ---\n",
    "    fig, ax1 = plt.subplots(figsize=(7, 5))\n",
    "    main_acc_color, asr_color, no_attack_color = 'mediumblue', 'crimson', 'dimgray'\n",
    "    legend_elements = []\n",
    "\n",
    "    # Plot \"No Attack\" baseline accuracy\n",
    "    ax1.axhline(y=no_attack_avg_acc, color=no_attack_color, linestyle='--', linewidth=2)\n",
    "    legend_elements.append(Line2D([0], [0], color=no_attack_color, linestyle='--', lw=2, label='Main Acc. (No Attack)'))\n",
    "\n",
    "    # Plot \"Backdoor\" Main Accuracy\n",
    "    ax1.plot(backdoor_attack_avg_perf['adv_rate'], backdoor_attack_avg_perf['FINAL_MAIN_ACC'],\n",
    "             color=main_acc_color, linestyle='-', marker='o', linewidth=2, markersize=7)\n",
    "    legend_elements.append(Line2D([0], [0], color=main_acc_color, marker='o', lw=2, label='Main Acc. (Backdoor)'))\n",
    "\n",
    "    ax1.set_xlabel('Adversary Rate', fontsize=12)\n",
    "    ax1.set_ylabel('Main Task Accuracy', color=main_acc_color, fontsize=12)\n",
    "    ax1.tick_params(axis='y', labelcolor=main_acc_color)\n",
    "    ax1.set_ylim(0, 1.05)\n",
    "\n",
    "    # Create second y-axis for ASR\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(backdoor_attack_avg_perf['adv_rate'], backdoor_attack_avg_perf['FINAL_ASR'],\n",
    "             color=asr_color, linestyle=':', marker='s', linewidth=2, markersize=7)\n",
    "    legend_elements.append(Line2D([0], [0], color=asr_color, marker='s', linestyle=':', lw=2, label='ASR (Backdoor)'))\n",
    "\n",
    "    ax2.set_ylabel('Attack Success Rate (ASR)', color=asr_color, fontsize=12)\n",
    "    ax2.tick_params(axis='y', labelcolor=asr_color)\n",
    "    ax2.set_ylim(0, 1.05)\n",
    "\n",
    "    # Final plot styling\n",
    "    ax1.set_xticks(ADV_RATES_TO_PLOT)\n",
    "    ax1.grid(True, which='major', linestyle=':', linewidth=0.7)\n",
    "    fig.legend(handles=legend_elements, loc='lower center', bbox_to_anchor=(0.5, -0.05), ncol=3, frameon=False)\n",
    "    plt.title(f'{AGG_METHOD_TO_ANALYZE}: Accuracy vs. ASR', fontsize=14, pad=20)\n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 1])\n",
    "\n",
    "    # Save the figure\n",
    "    FIGURE_SAVE_DIR.mkdir(exist_ok=True)\n",
    "    save_filename = FIGURE_SAVE_DIR / f\"accuracy_vs_asr-{AGG_METHOD_TO_ANALYZE}.pdf\"\n",
    "    fig.savefig(save_filename, bbox_inches='tight', format='pdf', dpi=300)\n",
    "    print(f\"✅ Figure saved to: {save_filename}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"❌ Plotting skipped. The summary DataFrame is empty or missing required columns.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "445dc0b19c37643d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CELL 3: Visualization 2 - Malicious Seller Selection Rate\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "# --- 1. Configuration & Style ---\n",
    "# TODO: Adjust these parameters for your specific analysis.\n",
    "\n",
    "# The root directory containing your experiment runs.\n",
    "ROOT_EXPERIMENT_DIR = './path/to/your/experiment_outputs'\n",
    "\n",
    "# A name for the aggregation method you're analyzing (for titles and filenames).\n",
    "AGG_METHOD_TO_ANALYZE = \"FedAvg\"\n",
    "\n",
    "# Define the specific adversary rates you want to display on the plot's x-axis.\n",
    "ADV_RATES_TO_PLOT = [0.1, 0.2, 0.3, 0.4]\n",
    "\n",
    "# The directory where you want to save the generated plot.\n",
    "FIGURE_SAVE_DIR = Path(\"./analysis_plots\")\n",
    "\n",
    "# Global font and style adjustments for a publication-quality plot\n",
    "mpl.rcParams.update({\n",
    "    'font.size': 16, 'axes.labelsize': 16, 'axes.titlesize': 18,\n",
    "    'xtick.labelsize': 14, 'ytick.labelsize': 14, 'legend.fontsize': 12,\n",
    "    'legend.title_fontsize': 13\n",
    "})\n",
    "\n",
    "# --- 2. Data Loading ---\n",
    "# This uses the functions defined in Cell 1.\n",
    "try:\n",
    "    all_runs = find_successful_runs(ROOT_EXPERIMENT_DIR)\n",
    "    summary_df = load_summary_df(all_runs)\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    summary_df = pd.DataFrame()\n",
    "\n",
    "# --- 3. Data Preprocessing for this Specific Plot ---\n",
    "if not summary_df.empty:\n",
    "    # Standardize metric column names\n",
    "    baseline_cols_map = {\n",
    "        rate: f'no_attack_designated_malicious_selection_rate_{rate}' for rate in ADV_RATES_TO_PLOT\n",
    "    }\n",
    "    summary_df = summary_df.rename(columns={\n",
    "        'avg_adversary_selection_rate': 'actual_malicious_selection_rate'\n",
    "    })\n",
    "\n",
    "    # ** IMPROVED LOGIC: Create category columns directly from flattened config **\n",
    "    adv_rate_col = 'experiment_adv_rate'\n",
    "    sybil_strategy_col = 'adversary_seller_config_sybil_sybil_strategy'\n",
    "\n",
    "    if adv_rate_col in summary_df.columns:\n",
    "        summary_df['adv_rate'] = summary_df[adv_rate_col]\n",
    "\n",
    "        # Define conditions for each group\n",
    "        conditions = [\n",
    "            summary_df[adv_rate_col] == 0,\n",
    "            (summary_df[adv_rate_col] > 0) & (summary_df[sybil_strategy_col] == 'mimic'),\n",
    "            summary_df[adv_rate_col] > 0\n",
    "        ]\n",
    "        # Define the corresponding group names\n",
    "        choices = ['Control (No Attack)', 'Attacker (Mimicry)', 'Attacker (Standard)']\n",
    "\n",
    "        summary_df['group_type'] = np.select(conditions, choices, default='Other')\n",
    "    else:\n",
    "        print(f\"Warning: Column '{adv_rate_col}' not found. Cannot generate plot.\")\n",
    "        summary_df = pd.DataFrame() # Invalidate df if key columns are missing\n",
    "\n",
    "# --- 4. Main Plotting Logic ---\n",
    "required_cols = list(baseline_cols_map.values()) + ['actual_malicious_selection_rate']\n",
    "if not summary_df.empty and any(col in summary_df.columns for col in required_cols):\n",
    "\n",
    "    # --- Data Aggregation ---\n",
    "    # Aggregate \"Control Group\" baseline\n",
    "    control_runs = summary_df[summary_df['group_type'] == 'Control (No Attack)']\n",
    "    baseline_points = {}\n",
    "    for adv_rate, col_name in baseline_cols_map.items():\n",
    "        if col_name in control_runs.columns:\n",
    "            mean_rate = control_runs[col_name].mean()\n",
    "            if pd.notna(mean_rate):\n",
    "                baseline_points[adv_rate] = mean_rate\n",
    "    baseline_series = pd.Series(baseline_points).sort_index()\n",
    "\n",
    "    # Aggregate \"Attacker Group\" performance\n",
    "    attacker_runs = summary_df[summary_df['group_type'].str.startswith('Attacker')]\n",
    "    attacker_avg_perf = attacker_runs.groupby(['adv_rate', 'group_type'])[\n",
    "        'actual_malicious_selection_rate'\n",
    "    ].mean().reset_index()\n",
    "\n",
    "    # --- Plotting ---\n",
    "    fig, ax = plt.subplots(figsize=(8, 5.5))\n",
    "\n",
    "    # Plot Control Group baseline\n",
    "    ax.plot(baseline_series.index, baseline_series.values, color='dimgray',\n",
    "            linestyle='--', marker='D', label='Control Group')\n",
    "\n",
    "    # Plot each attacker type\n",
    "    for group_name, group_data in attacker_avg_perf.groupby('group_type'):\n",
    "        style = {'marker': 'o', 'linestyle': '-'} if 'Standard' in group_name else {'marker': 's', 'linestyle': ':'}\n",
    "        color = 'orangered' if 'Standard' in group_name else 'darkviolet'\n",
    "        ax.plot(group_data['adv_rate'], group_data['actual_malicious_selection_rate'],\n",
    "                color=color, label=group_name, **style)\n",
    "\n",
    "    # Final plot styling\n",
    "    ax.set_xlabel('Adversary Rate')\n",
    "    ax.set_ylabel('Selection Rate')\n",
    "    ax.set_xticks(ADV_RATES_TO_PLOT)\n",
    "    ax.grid(True, which='major', linestyle=':', linewidth=0.6)\n",
    "    ax.legend(title='Seller Group', frameon=True)\n",
    "    ax.set_ylim(bottom=0, top=1.05)\n",
    "    plt.title(f'{AGG_METHOD_TO_ANALYZE}: Malicious Seller Selection Rate', pad=15)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    FIGURE_SAVE_DIR.mkdir(exist_ok=True)\n",
    "    save_filename = FIGURE_SAVE_DIR / f\"selection_rate-{AGG_METHOD_TO_ANALYZE}.pdf\"\n",
    "    fig.savefig(save_filename, bbox_inches='tight', format='pdf', dpi=300)\n",
    "    print(f\"✅ Figure saved to: {save_filename}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"❌ Plotting skipped. The DataFrame is empty or missing required selection rate columns.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2794df5f2a60ee9b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CELL 4: Visualization 3 - Defense Mechanism Performance (Detection vs. False Positives)\n",
    "\n",
    "# --- 1. Configuration & Style ---\n",
    "# TODO: Adjust these parameters for your specific analysis.\n",
    "ROOT_EXPERIMENT_DIR = './path/to/your/experiment_outputs'\n",
    "AGG_METHOD_TO_ANALYZE = \"FedAvg\"\n",
    "ADV_RATES_TO_PLOT = [0.1, 0.2, 0.3, 0.4]\n",
    "FIGURE_SAVE_DIR = Path(\"./analysis_plots\")\n",
    "# Global font styles can remain the same as the previous cell\n",
    "\n",
    "# --- 2. Data Loading ---\n",
    "try:\n",
    "    all_runs = find_successful_runs(ROOT_EXPERIMENT_DIR)\n",
    "    summary_df = load_summary_df(all_runs)\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    summary_df = pd.DataFrame()\n",
    "\n",
    "# --- 3. Data Preprocessing for this Specific Plot ---\n",
    "if not summary_df.empty:\n",
    "    # Standardize metric column names\n",
    "    summary_df = summary_df.rename(columns={\n",
    "        'adversary_detection_rate': 'DETECTION_RATE',\n",
    "        'false_positive_rate': 'FALSE_POSITIVE_RATE'\n",
    "    })\n",
    "\n",
    "    # Create the 'adv_rate' and 'attack_method' columns from loaded config\n",
    "    adv_rate_col = 'experiment_adv_rate'\n",
    "    if adv_rate_col in summary_df.columns:\n",
    "        summary_df['adv_rate'] = summary_df[adv_rate_col]\n",
    "        summary_df['attack_method'] = np.where(summary_df[adv_rate_col] > 0, 'Backdoor', 'No Attack')\n",
    "    else:\n",
    "        print(f\"Warning: Column '{adv_rate_col}' not found. Cannot generate plot.\")\n",
    "        summary_df = pd.DataFrame()\n",
    "\n",
    "# --- 4. Main Plotting Logic ---\n",
    "required_cols = ['DETECTION_RATE', 'FALSE_POSITIVE_RATE']\n",
    "if not summary_df.empty and all(col in summary_df.columns for col in required_cols):\n",
    "\n",
    "    # --- Data Aggregation ---\n",
    "    no_attack_runs = summary_df[summary_df['attack_method'] == 'No Attack']\n",
    "    attacker_runs = summary_df[summary_df['attack_method'] == 'Backdoor']\n",
    "\n",
    "    # ** REFINED LOGIC: More direct calculation for the baseline FPR **\n",
    "    # Since all 'No Attack' runs should have a similar FPR, we can average them all together.\n",
    "    no_attack_fpr = no_attack_runs['FALSE_POSITIVE_RATE'].mean()\n",
    "\n",
    "    # Average the detection rate for attackers at each adversary rate\n",
    "    detection_rate_avg = attacker_runs.groupby('adv_rate')['DETECTION_RATE'].mean().reset_index()\n",
    "\n",
    "    # --- Plotting ---\n",
    "    fig, ax = plt.subplots(figsize=(8, 5.5))\n",
    "\n",
    "    # Plot False Positive Rate (FPR) as a constant baseline\n",
    "    ax.axhline(y=no_attack_fpr, color='darkorange', linestyle='--', marker='^', label='False Positive Rate (FPR)')\n",
    "\n",
    "    # Plot Adversary Detection Rate (TPR)\n",
    "    ax.plot(detection_rate_avg['adv_rate'], detection_rate_avg['DETECTION_RATE'],\n",
    "            color='teal', linestyle='-', marker='o', label='Adversary Detection Rate')\n",
    "\n",
    "    # Final plot styling\n",
    "    ax.set_xlabel('Adversary Rate')\n",
    "    ax.set_ylabel('Rate')\n",
    "    ax.set_xticks(ADV_RATES_TO_PLOT)\n",
    "    ax.grid(True, which='major', linestyle=':', linewidth=0.6)\n",
    "    ax.legend(frameon=True)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    plt.title(f'{AGG_METHOD_TO_ANALYZE}: Defense Performance', pad=15)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    FIGURE_SAVE_DIR.mkdir(exist_ok=True)\n",
    "    save_filename = FIGURE_SAVE_DIR / f\"defense_performance-{AGG_METHOD_TO_ANALYZE}.pdf\"\n",
    "    fig.savefig(save_filename, bbox_inches='tight', format='pdf', dpi=300)\n",
    "    print(f\"✅ Figure saved to: {save_filename}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"❌ Plotting skipped. The DataFrame is empty or missing required columns (DETECTION_RATE, FALSE_POSITIVE_RATE).\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df8a18370f0ab2ed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CELL 5: High-Granularity Comparative Analysis Plot\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 1. Configuration for High-Granularity Plotting ---\n",
    "# TODO: Adjust these parameters to create your desired comparison.\n",
    "\n",
    "# Define the specific filters to isolate the runs you want to analyze.\n",
    "# The keys must match the column names in your summary_df (including flattened config keys).\n",
    "# Example below compares FedAvg vs. FLAME on the CIFAR-10 dataset.\n",
    "FILTERS = {\n",
    "    'experiment_dataset_name': 'cifar10',\n",
    "    # 'experiment_adv_rate': 0.2  # You could uncomment to lock in a specific adversary rate\n",
    "}\n",
    "\n",
    "# Define the column you want to use to separate lines on the plot (the comparison variable).\n",
    "# This MUST be a column in your summary_df.\n",
    "# Example: 'aggregation_name' will create separate lines for 'FedAvg', 'FLAME', etc.\n",
    "HUE_COLUMN = 'aggregation_name'\n",
    "\n",
    "# Define the columns for your plot's axes.\n",
    "X_AXIS_COLUMN = 'experiment_adv_rate' # e.g., 'experiment_adv_rate'\n",
    "Y_AXIS_COLUMN = 'test_accuracy'         # e.g., 'test_accuracy', 'attack_success_rate'\n",
    "\n",
    "# --- Plot Customization ---\n",
    "PLOT_TITLE = \"Performance Comparison: FedAvg vs. FLAME on CIFAR-10\"\n",
    "X_AXIS_LABEL = \"Adversary Rate\"\n",
    "Y_AXIS_LABEL = \"Test Accuracy\"\n",
    "SAVE_FILENAME = \"comparison_fedavg_vs_flame_cifar10\"\n",
    "FIGURE_SAVE_DIR = Path(\"./analysis_plots\")\n",
    "\n",
    "# --- 2. Data Loading and Filtering ---\n",
    "# This uses the functions from Cell 1 and the full summary_df loaded in Cell 2/3/4.\n",
    "if 'summary_df' not in locals() or summary_df.empty:\n",
    "    print(\"Please run the previous cells to load the 'summary_df' DataFrame first.\")\n",
    "else:\n",
    "    filtered_df = summary_df.copy()\n",
    "\n",
    "    # Apply all the filters defined in the configuration dictionary\n",
    "    for key, value in FILTERS.items():\n",
    "        if key in filtered_df.columns:\n",
    "            filtered_df = filtered_df[filtered_df[key] == value]\n",
    "        else:\n",
    "            print(f\"Warning: Filter key '{key}' not found in DataFrame columns. Skipping filter.\")\n",
    "\n",
    "    # --- 3. Data Aggregation for Plotting ---\n",
    "    if not filtered_df.empty:\n",
    "        # Group by the x-axis and the comparison variable (hue), then average the results\n",
    "        # This handles averaging across different random seeds automatically.\n",
    "        plot_data = filtered_df.groupby([X_AXIS_COLUMN, HUE_COLUMN])[Y_AXIS_COLUMN].mean().reset_index()\n",
    "\n",
    "        # --- 4. Main Plotting Logic ---\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        sns.lineplot(\n",
    "            data=plot_data,\n",
    "            x=X_AXIS_COLUMN,\n",
    "            y=Y_AXIS_COLUMN,\n",
    "            hue=HUE_COLUMN,\n",
    "            style=HUE_COLUMN, # Use different line styles for clarity\n",
    "            markers=True,     # Add markers to each data point\n",
    "            markersize=8,\n",
    "            linewidth=2.5\n",
    "        )\n",
    "\n",
    "        # --- Final Plot Styling ---\n",
    "        plt.xlabel(X_AXIS_LABEL)\n",
    "        plt.ylabel(Y_AXIS_LABEL)\n",
    "        plt.title(PLOT_TITLE, pad=15)\n",
    "        plt.grid(True, which='major', linestyle='--', linewidth=0.5)\n",
    "        plt.legend(title=HUE_COLUMN.replace('_', ' ').title())\n",
    "        plt.ylim(bottom=0)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the figure\n",
    "        FIGURE_SAVE_DIR.mkdir(exist_ok=True)\n",
    "        save_path = FIGURE_SAVE_DIR / f\"{SAVE_FILENAME}.pdf\"\n",
    "        plt.savefig(save_path, bbox_inches='tight', format='pdf', dpi=300)\n",
    "        print(f\"✅ Figure saved to: {save_path}\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(\"❌ Plotting skipped. No data remaining after applying filters.\")\n",
    "        print(f\"   Applied filters: {FILTERS}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CELL 6: Visualization 4 - Convergence Cost to Accuracy Milestones\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "# --- 1. Configuration & Style ---\n",
    "# TODO: Adjust these parameters for your specific analysis.\n",
    "\n",
    "# The root directory containing your experiment runs.\n",
    "ROOT_EXPERIMENT_DIR = './path/to/your/experiment_outputs'\n",
    "\n",
    "# A name for the aggregation method you're analyzing (for titles and filenames).\n",
    "AGG_METHOD_TO_ANALYIZE = \"FedAvg\"\n",
    "\n",
    "# Define the accuracy milestones you want to plot.\n",
    "# IMPORTANT: These must correspond to columns saved in your final_metrics.json\n",
    "MILESTONE_ACC_LABELS = [\"70\", \"80\", \"85\"]\n",
    "\n",
    "# Define a fixed adversary rate to compare attack scenarios.\n",
    "FIXED_ADV_RATE_FOR_COMPARISON = 0.3\n",
    "\n",
    "# The directory where you want to save the generated plot.\n",
    "FIGURE_SAVE_DIR = Path(\"./analysis_plots\")\n",
    "\n",
    "# --- 2. Data Loading ---\n",
    "# This uses the functions from Cell 1.\n",
    "try:\n",
    "    all_runs = find_successful_runs(ROOT_EXPERIMENT_DIR)\n",
    "    summary_df = load_summary_df(all_runs)\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    summary_df = pd.DataFrame()\n",
    "\n",
    "# --- 3. Data Preprocessing for this Specific Plot ---\n",
    "if not summary_df.empty:\n",
    "    # ** IMPROVED LOGIC: Auto-detect which set of milestone columns is present **\n",
    "    milestone_cols = [f'cost_to_{label}_percent_accuracy' for label in MILESTONE_ACC_LABELS]\n",
    "    y_axis_label = 'Cost (e.g., Rounds) to Converge'\n",
    "\n",
    "    # Check if the primary naming convention exists, if not, try the fallback.\n",
    "    if not all(col in summary_df.columns for col in milestone_cols):\n",
    "        milestone_cols = [f'ROUNDS_TO_{label}ACC' for label in MILESTONE_ACC_LABELS]\n",
    "        y_axis_label = 'Rounds to Converge'\n",
    "        print(\"ℹ️ Using 'ROUNDS_TO_...' naming convention for milestone costs.\")\n",
    "    else:\n",
    "        print(\"ℹ️ Using 'cost_to_...' naming convention for milestone costs.\")\n",
    "\n",
    "    # Invalidate DataFrame if milestone columns are still not found\n",
    "    if not all(col in summary_df.columns for col in milestone_cols):\n",
    "        print(\"❌ Preprocessing failed: Could not find a complete set of milestone columns.\")\n",
    "        summary_df = pd.DataFrame() # Mark for skipping plot\n",
    "\n",
    "    # ** IMPROVED LOGIC: Use flattened config for robust filtering **\n",
    "    adv_rate_col = 'experiment_adv_rate'\n",
    "    sybil_strategy_col = 'adversary_seller_config_sybil_sybil_strategy'\n",
    "\n",
    "    if adv_rate_col in summary_df.columns:\n",
    "        summary_df['adv_rate'] = summary_df[adv_rate_col]\n",
    "\n",
    "        conditions = [\n",
    "            summary_df[adv_rate_col] == 0,\n",
    "            (np.isclose(summary_df[adv_rate_col], FIXED_ADV_RATE_FOR_COMPARISON)) & (summary_df[sybil_strategy_col] == 'mimic'),\n",
    "            (np.isclose(summary_df[adv_rate_col], FIXED_ADV_RATE_FOR_COMPARISON))\n",
    "        ]\n",
    "        choices = ['No Attack', 'Backdoor (Sybil)', 'Backdoor (Standard)']\n",
    "        summary_df['Condition'] = np.select(conditions, choices, default='Other')\n",
    "    else:\n",
    "        print(f\"Warning: Column '{adv_rate_col}' not found. Cannot generate plot.\")\n",
    "        summary_df = pd.DataFrame()\n",
    "\n",
    "# --- 4. Main Plotting Logic ---\n",
    "if not summary_df.empty:\n",
    "\n",
    "    # --- Data Aggregation ---\n",
    "    # Filter for relevant conditions and average results across seeds\n",
    "    data_to_plot = summary_df[summary_df['Condition'].isin(['No Attack', 'Backdoor (Standard)', 'Backdoor (Sybil)'])]\n",
    "    avg_costs = data_to_plot.groupby('Condition')[milestone_cols].mean()\n",
    "\n",
    "    # ** IMPROVED LOGIC: Use pd.melt to transform data for easy plotting **\n",
    "    # This converts wide data (cost_to_70, cost_to_80) to long data needed for seaborn\n",
    "    plot_df = avg_costs.reset_index().melt(\n",
    "        id_vars='Condition',\n",
    "        var_name='Milestone_col',\n",
    "        value_name='Cost'\n",
    "    )\n",
    "    # Create clean labels for the x-axis\n",
    "    plot_df['Milestone'] = plot_df['Milestone_col'].apply(lambda x: f\"{x.split('_')[2]}% Acc\")\n",
    "\n",
    "    # --- Plotting Grouped Bar Chart ---\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    palette = {'No Attack': 'dimgray', 'Backdoor (Standard)': 'orangered', 'Backdoor (Sybil)': 'darkviolet'}\n",
    "    hue_order = ['No Attack', 'Backdoor (Standard)', 'Backdoor (Sybil)']\n",
    "\n",
    "    sns.barplot(data=plot_df, x='Milestone', y='Cost', hue='Condition',\n",
    "                order=[f\"{label}% Acc\" for label in MILESTONE_ACC_LABELS],\n",
    "                hue_order=hue_order, palette=palette)\n",
    "\n",
    "    plt.xlabel('Target Accuracy Milestone')\n",
    "    plt.ylabel(y_axis_label)\n",
    "    plt.grid(True, which='major', linestyle=':', linewidth=0.7, axis='y')\n",
    "    plt.legend(title='Attack Scenario')\n",
    "    plt.title(f'{AGG_METHOD_TO_ANALYZE}: Convergence Cost\\n(Attacks at {FIXED_ADV_RATE_FOR_COMPARISON*100:.0f}% Rate)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    save_filename = FIGURE_SAVE_DIR / f\"milestone_cost-{AGG_METHOD_TO_ANALYZE}.pdf\"\n",
    "    fig.savefig(save_filename, bbox_inches='tight', format='pdf', dpi=300)\n",
    "    print(f\"✅ Figure saved to: {save_filename}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"❌ Plotting skipped. The DataFrame is empty or missing required milestone cost columns.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "252df9faaed31646"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from pathlib import Path\n",
    "\n",
    "# This script assumes 'final_metrics_df' is available from the first cell.\n",
    "\n",
    "# --- 1. Configuration & Style ---\n",
    "FIGURE_SAVE_DIR = Path(\"./analysis_plots\")\n",
    "FIGURE_SAVE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Global font and style adjustments\n",
    "mpl.rcParams.update({\n",
    "    'font.size': 16, 'axes.labelsize': 16, 'axes.titlesize': 18,\n",
    "    'xtick.labelsize': 14, 'ytick.labelsize': 14, 'legend.fontsize': 12,\n",
    "    'legend.title_fontsize': 13, 'pdf.fonttype': 42, 'ps.fonttype': 42,\n",
    "})\n",
    "\n",
    "# --- 2. Helper Function to Save Figures ---\n",
    "def save_figure_as_pdf(fig, output_directory: Path, base_filename: str, **details):\n",
    "    \"\"\"Saves a matplotlib figure to a PDF with a descriptive filename.\"\"\"\n",
    "    filename_parts = [base_filename]\n",
    "    for key, value in details.items():\n",
    "        clean_value = str(value).replace('.', '_')\n",
    "        filename_parts.append(f\"{key}_{clean_value}\")\n",
    "    final_filename = f\"{'-'.join(filename_parts)}.pdf\"\n",
    "    save_path = output_directory / final_filename\n",
    "    try:\n",
    "        fig.savefig(save_path, bbox_inches='tight', format='pdf', dpi=300)\n",
    "        print(f\"✅ Figure saved successfully to: {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not save figure: {e}\")\n",
    "\n",
    "# --- 3. Data Preprocessing & Filtering ---\n",
    "summary_df = final_metrics_df.copy()\n",
    "\n",
    "# --- FILTERING CONFIGURATION ---\n",
    "TARGET_DATASET = 'CIFAR-10'  # or None\n",
    "TARGET_MODEL = None      # or None\n",
    "TARGET_AGG_METHOD = None # or None\n",
    "REPRESENTATIVE_ADV_RATE_FOR_ATTACK = 0.3 # Adv rate for attack scenarios\n",
    "# ---\n",
    "\n",
    "# Apply filters\n",
    "print(\"--- Applying Filters for Cost Composition Plot ---\")\n",
    "if TARGET_DATASET and 'param_dataset_name' in summary_df.columns:\n",
    "    summary_df = summary_df[summary_df['param_dataset_name'] == TARGET_DATASET]\n",
    "    print(f\"✅ Filtered for Dataset: {TARGET_DATASET}\")\n",
    "if TARGET_MODEL and 'param_model_structure' in summary_df.columns:\n",
    "    summary_df = summary_df[summary_df['param_model_structure'] == TARGET_MODEL]\n",
    "    print(f\"✅ Filtered for Model: {TARGET_MODEL}\")\n",
    "if TARGET_AGG_METHOD and 'param_aggregation_name' in summary_df.columns:\n",
    "    summary_df = summary_df[summary_df['param_aggregation_name'] == TARGET_AGG_METHOD]\n",
    "    print(f\"✅ Filtered for Aggregation: {TARGET_AGG_METHOD}\")\n",
    "\n",
    "# Create helper columns\n",
    "if 'param_adversary_seller_config' in summary_df.columns:\n",
    "    summary_df['ATTACK_METHOD'] = np.where(\n",
    "        summary_df['param_adversary_seller_config'].str.contains(\"'is_adversary': True\"), 'Backdoor', 'No Attack')\n",
    "    summary_df['IS_SYBIL'] = np.where(\n",
    "        summary_df['param_adversary_seller_config'].str.contains(\"'sybil_strategy': 'mimic'\"), 'mimic', 'False')\n",
    "    if 'param_num_adversaries' in summary_df.columns and 'param_num_sellers' in summary_df.columns:\n",
    "         summary_df['ADV_RATE'] = pd.to_numeric(summary_df['param_num_adversaries']) / pd.to_numeric(summary_df['param_num_sellers'])\n",
    "    else: summary_df['ADV_RATE'] = 0.0\n",
    "else:\n",
    "    print(\"⚠️ Warning: Could not determine ATTACK_METHOD/ADV_RATE. Using dummy values.\")\n",
    "    summary_df['ATTACK_METHOD'] = 'Backdoor'; summary_df['ADV_RATE'] = 0.0; summary_df['IS_SYBIL'] = 'False'\n",
    "\n",
    "\n",
    "# --- 4. Main Plotting Logic for Cost Composition ---\n",
    "cost_benign_col = 'avg_cost_per_round_benign'\n",
    "cost_mal_col = 'avg_cost_per_round_malicious'\n",
    "\n",
    "if not (cost_benign_col in summary_df.columns and cost_mal_col in summary_df.columns):\n",
    "    print(f\"❌ Plotting skipped. Cost columns ('{cost_benign_col}', '{cost_mal_col}') not found.\")\n",
    "elif summary_df[[cost_benign_col, cost_mal_col]].isna().all().all():\n",
    "    print(f\"❌ Plotting skipped. Cost columns are entirely NaN.\")\n",
    "else:\n",
    "    # --- Prepare Data for the 3 Scenarios ---\n",
    "    # 1. No Attack Scenario\n",
    "    no_attack_data = summary_df[summary_df['ATTACK_METHOD'] == 'No Attack']\n",
    "    avg_no_attack_benign = no_attack_data[cost_benign_col].mean() if not no_attack_data.empty else 0\n",
    "    avg_no_attack_mal = no_attack_data[cost_mal_col].mean() if not no_attack_data.empty else 0\n",
    "\n",
    "    # 2. Backdoor Attack (Standard)\n",
    "    std_backdoor_data = summary_df[\n",
    "        (summary_df['ATTACK_METHOD'] == 'Backdoor') & (summary_df['IS_SYBIL'] == 'False') &\n",
    "        (np.isclose(summary_df['ADV_RATE'], REPRESENTATIVE_ADV_RATE_FOR_ATTACK))]\n",
    "    avg_std_backdoor_benign = std_backdoor_data[cost_benign_col].mean() if not std_backdoor_data.empty else 0\n",
    "    avg_std_backdoor_mal = std_backdoor_data[cost_mal_col].mean() if not std_backdoor_data.empty else 0\n",
    "\n",
    "    # 3. Backdoor Attack (Sybil/Mimicry)\n",
    "    mimic_backdoor_data = summary_df[\n",
    "        (summary_df['ATTACK_METHOD'] == 'Backdoor') & (summary_df['IS_SYBIL'] == 'mimic') &\n",
    "        (np.isclose(summary_df['ADV_RATE'], REPRESENTATIVE_ADV_RATE_FOR_ATTACK))]\n",
    "    avg_mimic_backdoor_benign = mimic_backdoor_data[cost_benign_col].mean() if not mimic_backdoor_data.empty else 0\n",
    "    avg_mimic_backdoor_mal = mimic_backdoor_data[cost_mal_col].mean() if not mimic_backdoor_data.empty else 0\n",
    "\n",
    "    # --- Data for Stacked Bar Chart ---\n",
    "    scenarios = ['No Attack', 'Backdoor (Std)', 'Backdoor (Sybil)']\n",
    "    benign_costs = [avg_no_attack_benign, avg_std_backdoor_benign, avg_mimic_backdoor_benign]\n",
    "    malicious_costs = [avg_no_attack_mal, avg_std_backdoor_mal, avg_mimic_backdoor_mal]\n",
    "\n",
    "    # --- Plotting ---\n",
    "    if any(c > 0 for c in benign_costs + malicious_costs):\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        x_pos = np.arange(len(scenarios))\n",
    "\n",
    "        ax.bar(x_pos, benign_costs, label='Cost from Benign Sellers', color='cornflowerblue')\n",
    "        ax.bar(x_pos, malicious_costs, bottom=benign_costs, label='Cost from Malicious Sellers', color='salmon')\n",
    "\n",
    "        ax.set_xlabel('Attack Scenario')\n",
    "        ax.set_ylabel('Average Cost per Round')\n",
    "        ax.set_xticks(x_pos)\n",
    "        ax.set_xticklabels(scenarios)\n",
    "        ax.grid(True, which='major', linestyle=':', linewidth=0.7, axis='y')\n",
    "        ax.set_axisbelow(True)\n",
    "        ax.legend(title='Cost Component')\n",
    "\n",
    "        agg_method = summary_df['param_aggregation_name'].iloc[0] if 'param_aggregation_name' in summary_df.columns else \"Unknown\"\n",
    "        plt.title(f'{agg_method}: Composition of Cost per Round\\n(Attacks at {REPRESENTATIVE_ADV_RATE_FOR_ATTACK*100:.0f}% Adversary Rate)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        save_figure_as_pdf(fig=fig, output_directory=FIGURE_SAVE_DIR, base_filename=\"cost_composition\", aggregation=agg_method)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"ℹ️ Not enough data to plot Cost Composition bar chart after filtering.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40c1ba40e795f487"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CELL 7: Visualization 5 - Composition of Cost per Round\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "# --- 1. Configuration for Visualization ---\n",
    "# TODO: Adjust these parameters for your specific analysis.\n",
    "\n",
    "# The root directory containing your experiment runs.\n",
    "ROOT_EXPERIMENT_DIR = './path/to/your/experiment_outputs'\n",
    "\n",
    "# A name for the aggregation method you're analyzing (for titles and filenames).\n",
    "AGG_METHOD_TO_ANALYZE = \"FedAvg\"\n",
    "\n",
    "# Define the fixed adversary rate to compare attack scenarios.\n",
    "REPRESENTATIVE_ADV_RATE_FOR_ATTACK = 0.3\n",
    "\n",
    "# The directory where you want to save the generated plot.\n",
    "FIGURE_SAVE_DIR = Path(\"./analysis_plots\")\n",
    "\n",
    "# --- 2. Data Loading ---\n",
    "# This uses the functions from Cell 1.\n",
    "try:\n",
    "    all_runs = find_successful_runs(ROOT_EXPERIMENT_DIR)\n",
    "    summary_df = load_summary_df(all_runs)\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    summary_df = pd.DataFrame()\n",
    "\n",
    "# --- 3. Data Preprocessing for this Specific Plot ---\n",
    "if not summary_df.empty:\n",
    "    # Standardize metric column names\n",
    "    summary_df = summary_df.rename(columns={\n",
    "        'avg_cost_per_round_benign': 'cost_benign',\n",
    "        'avg_cost_per_round_malicious': 'cost_malicious'\n",
    "    })\n",
    "\n",
    "    # ** IMPROVED LOGIC: Use flattened config for robust filtering and grouping **\n",
    "    adv_rate_col = 'experiment_adv_rate'\n",
    "    sybil_strategy_col = 'adversary_seller_config_sybil_sybil_strategy'\n",
    "\n",
    "    if adv_rate_col in summary_df.columns:\n",
    "        summary_df['adv_rate'] = summary_df[adv_rate_col]\n",
    "\n",
    "        conditions = [\n",
    "            summary_df[adv_rate_col] == 0,\n",
    "            (np.isclose(summary_df[adv_rate_col], REPRESENTATIVE_ADV_RATE_FOR_ATTACK)) & (summary_df.get(sybil_strategy_col) == 'mimic'),\n",
    "            (np.isclose(summary_df[adv_rate_col], REPRESENTATIVE_ADV_RATE_FOR_ATTACK))\n",
    "        ]\n",
    "        choices = ['No Attack', 'Backdoor (Sybil)', 'Backdoor (Standard)']\n",
    "        summary_df['Condition'] = np.select(conditions, choices, default='Other')\n",
    "    else:\n",
    "        print(f\"Warning: Column '{adv_rate_col}' not found. Cannot generate plot.\")\n",
    "        summary_df = pd.DataFrame()\n",
    "\n",
    "# --- 4. Main Plotting Logic ---\n",
    "required_cols = ['cost_benign', 'cost_malicious']\n",
    "if not summary_df.empty and all(col in summary_df.columns for col in required_cols):\n",
    "\n",
    "    # --- Data Aggregation ---\n",
    "    # ** IMPROVED LOGIC: Use groupby().mean() for cleaner aggregation **\n",
    "    data_to_plot = summary_df[summary_df['Condition'].isin(['No Attack', 'Backdoor (Standard)', 'Backdoor (Sybil)'])]\n",
    "\n",
    "    aggregated_costs = data_to_plot.groupby('Condition')[required_cols].mean()\n",
    "\n",
    "    # Ensure a consistent order for plotting\n",
    "    plot_order = ['No Attack', 'Backdoor (Standard)', 'Backdoor (Sybil)']\n",
    "    aggregated_costs = aggregated_costs.reindex(plot_order)\n",
    "\n",
    "    # --- Plotting Stacked Bar Chart ---\n",
    "    if not aggregated_costs.empty:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "        # Plot the bars\n",
    "        ax.bar(aggregated_costs.index, aggregated_costs['cost_benign'], label='Cost from Benign Sellers', color='cornflowerblue')\n",
    "        ax.bar(aggregated_costs.index, aggregated_costs['cost_malicious'], bottom=aggregated_costs['cost_benign'], label='Cost from Malicious Sellers', color='salmon')\n",
    "\n",
    "        # Final plot styling\n",
    "        ax.set_xlabel('Attack Scenario')\n",
    "        ax.set_ylabel('Average Cost per Round')\n",
    "        ax.grid(True, which='major', linestyle=':', linewidth=0.7, axis='y')\n",
    "        ax.legend(title='Cost Component')\n",
    "        plt.title(f'{AGG_METHOD_TO_ANALYZE}: Composition of Cost per Round\\n(Attacks at {REPRESENTATIVE_ADV_RATE_FOR_ATTACK*100:.0f}% Rate)')\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        save_filename = FIGURE_SAVE_DIR / f\"cost_composition-{AGG_METHOD_TO_ANALYZE}.pdf\"\n",
    "        fig.savefig(save_filename, bbox_inches='tight', format='pdf', dpi=300)\n",
    "        print(f\"✅ Figure saved to: {save_filename}\")\n",
    "\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"ℹ️ Not enough data to create the Cost Composition bar chart after filtering.\")\n",
    "else:\n",
    "    print(\"❌ Plotting skipped. The DataFrame is empty or missing required cost columns.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "daa9d1b542b31cb2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# geni"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26ade3f415b569df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CELL 8: Visualization 6 - Fairness of Benign Seller Payments (Gini Coefficient)\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# --- 1. Configuration for Visualization ---\n",
    "# TODO: Adjust these parameters for your specific analysis.\n",
    "\n",
    "# The root directory containing your experiment runs.\n",
    "ROOT_EXPERIMENT_DIR = './path/to/your/experiment_outputs'\n",
    "\n",
    "# Filter for a specific aggregation method to analyze.\n",
    "AGG_METHOD_TO_ANALYZE = \"FedAvg\"\n",
    "\n",
    "# Define the adversary rates to display on the plot's x-axis.\n",
    "ADV_RATES_TO_PLOT = [0.1, 0.2, 0.3, 0.4]\n",
    "\n",
    "# The directory where you want to save the generated plot.\n",
    "FIGURE_SAVE_DIR = Path(\"./analysis_plots\")\n",
    "\n",
    "# --- 2. Data Loading ---\n",
    "# This uses the functions from Cell 1.\n",
    "try:\n",
    "    all_runs = find_successful_runs(ROOT_EXPERIMENT_DIR)\n",
    "    summary_df = load_summary_df(all_runs)\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    summary_df = pd.DataFrame()\n",
    "\n",
    "# --- 3. Data Preprocessing for this Specific Plot ---\n",
    "if not summary_df.empty:\n",
    "    # Standardize metric column name\n",
    "    summary_df = summary_df.rename(columns={'avg_benign_payment_gini': 'BENIGN_GINI'})\n",
    "\n",
    "    # ** IMPROVED LOGIC: Use flattened config for robust filtering and grouping **\n",
    "    adv_rate_col = 'experiment_adv_rate'\n",
    "    sybil_strategy_col = 'adversary_seller_config_sybil_sybil_strategy'\n",
    "    agg_method_col = 'aggregation_name'\n",
    "\n",
    "    # Filter for the target aggregation method first\n",
    "    if AGG_METHOD_TO_ANALYZE and agg_method_col in summary_df.columns:\n",
    "        summary_df = summary_df[summary_df[agg_method_col] == AGG_METHOD_TO_ANALYZE]\n",
    "\n",
    "    if adv_rate_col in summary_df.columns:\n",
    "        summary_df['adv_rate'] = summary_df[adv_rate_col]\n",
    "\n",
    "        conditions = [\n",
    "            summary_df[adv_rate_col] == 0,\n",
    "            (summary_df[adv_rate_col] > 0) & (summary_df.get(sybil_strategy_col) == 'mimic'),\n",
    "            summary_df[adv_rate_col] > 0\n",
    "        ]\n",
    "        choices = ['No Attack', 'Backdoor (Sybil)', 'Backdoor (Standard)']\n",
    "        summary_df['Condition'] = np.select(conditions, choices, default='Other')\n",
    "    else:\n",
    "        print(f\"Warning: Column '{adv_rate_col}' not found. Cannot generate plot.\")\n",
    "        summary_df = pd.DataFrame()\n",
    "\n",
    "# --- 4. Main Plotting Logic ---\n",
    "if not summary_df.empty and 'BENIGN_GINI' in summary_df.columns:\n",
    "\n",
    "    # --- Data Aggregation ---\n",
    "    no_attack_runs = summary_df[summary_df['Condition'] == 'No Attack']\n",
    "    attacker_runs = summary_df[summary_df['Condition'].str.startswith('Backdoor')]\n",
    "\n",
    "    # Calculate the baseline Gini for the \"No Attack\" scenario\n",
    "    no_attack_avg_gini = no_attack_runs['BENIGN_GINI'].mean()\n",
    "\n",
    "    # Calculate the average Gini for each attacker type at each adversary rate\n",
    "    attacker_avg_perf = attacker_runs.groupby(['adv_rate', 'Condition'])['BENIGN_GINI'].mean().reset_index()\n",
    "\n",
    "    # --- Plotting ---\n",
    "    fig, ax = plt.subplots(figsize=(8, 5.5))\n",
    "    legend_elements = []\n",
    "\n",
    "    # Plot \"No Attack\" baseline\n",
    "    ax.axhline(y=no_attack_avg_gini, color='dimgray', linestyle='--', linewidth=2)\n",
    "    legend_elements.append(Line2D([0], [0], color='dimgray', linestyle='--', lw=2, label='No Attack'))\n",
    "\n",
    "    # Plot each attacker type\n",
    "    styles = {'Backdoor (Standard)': {'color': 'orangered', 'marker': 'o', 'linestyle': '-'},\n",
    "              'Backdoor (Sybil)': {'color': 'darkviolet', 'marker': 's', 'linestyle': ':'}}\n",
    "\n",
    "    for name, group in attacker_avg_perf.groupby('Condition'):\n",
    "        ax.plot(group['adv_rate'], group['BENIGN_GINI'], label=name, **styles[name])\n",
    "\n",
    "    # Final plot styling\n",
    "    ax.set_xlabel('Adversary Rate')\n",
    "    ax.set_ylabel('Benign Seller Payment Gini\\n(0=Equal, 1=Unequal)')\n",
    "    ax.set_xticks(ADV_RATES_TO_PLOT)\n",
    "    ax.grid(True, which='major', linestyle=':', linewidth=0.6)\n",
    "    ax.legend(title='Attack Scenario')\n",
    "    ax.set_ylim(0, 1.0) # Gini is bounded by 0 and 1\n",
    "    plt.title(f'{AGG_METHOD_TO_ANALYZE}: Fairness of Benign Seller Payments', pad=15)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    FIGURE_SAVE_DIR.mkdir(exist_ok=True)\n",
    "    save_filename = FIGURE_SAVE_DIR / f\"gini_fairness-{AGG_METHOD_TO_ANALYZE}.pdf\"\n",
    "    fig.savefig(save_filename, bbox_inches='tight', format='pdf', dpi=300)\n",
    "    print(f\"✅ Figure saved to: {save_filename}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"❌ Plotting skipped. The DataFrame is empty or missing the 'BENIGN_GINI' column.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4ca9e91a2dbd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CELL 9: Visualization 7 - Temporal Evolution of Seller Contributions\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 1. Configuration for Visualization ---\n",
    "# TODO: Adjust these parameters for your specific analysis.\n",
    "\n",
    "# The root directory containing your experiment runs.\n",
    "ROOT_EXPERIMENT_DIR = './path/to/your/experiment_outputs'\n",
    "\n",
    "# Filter for a specific aggregation method to analyze.\n",
    "AGG_METHOD_TO_ANALYZE = \"FedAvg\"\n",
    "\n",
    "# Define the metric you want to plot over time.\n",
    "# 'gradient_norm' is excellent for discovery. 'train_loss' is another good option.\n",
    "Y_AXIS_COLUMN = 'gradient_norm'\n",
    "Y_AXIS_LABEL = 'Gradient Norm (L2)'\n",
    "PLOT_TITLE = f'{AGG_METHOD_TO_ANALYZE}: Gradient Norm Evolution'\n",
    "SAVE_FILENAME = f'temporal_gradient_norm-{AGG_METHOD_TO_ANALYZE}'\n",
    "\n",
    "# The directory where you want to save the generated plot.\n",
    "FIGURE_SAVE_DIR = Path(\"./analysis_plots\")\n",
    "\n",
    "# --- 2. Data Loading ---\n",
    "# This uses the functions from Cell 1.\n",
    "try:\n",
    "    all_runs = find_successful_runs(ROOT_EXPERIMENT_DIR)\n",
    "    seller_df = load_seller_analysis_df(all_runs)\n",
    "    # Also load summary_df to get config params for filtering\n",
    "    summary_df = load_summary_df(all_runs)\n",
    "    # Merge the config into the seller_df for easy filtering\n",
    "    seller_df = pd.merge(seller_df, summary_df[['run_id', 'seed', 'aggregation_name']], on=['run_id', 'seed'])\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    seller_df = pd.DataFrame()\n",
    "\n",
    "# --- 3. Data Preprocessing and Filtering ---\n",
    "if not seller_df.empty:\n",
    "    # Filter for the target aggregation method\n",
    "    if AGG_METHOD_TO_ANALYZE and 'aggregation_name' in seller_df.columns:\n",
    "        seller_df = seller_df[seller_df['aggregation_name'] == AGG_METHOD_TO_ANALYZE]\n",
    "\n",
    "    # ** IMPROVED LOGIC: Use the existing 'seller_type' column **\n",
    "    # The load function already creates this, but we'll add the specific mimicry case\n",
    "    sybil_strategy_col = 'adversary_seller_config_sybil_sybil_strategy'\n",
    "    if sybil_strategy_col in summary_df.columns:\n",
    "         # Get run_id, seed pairs for mimicry runs\n",
    "        mimic_runs = summary_df[summary_df[sybil_strategy_col] == 'mimic'][['run_id', 'seed']]\n",
    "        if not mimic_runs.empty:\n",
    "            # Create a multi-index to identify mimicry runs efficiently\n",
    "            mimic_idx = pd.MultiIndex.from_frame(mimic_runs)\n",
    "            seller_idx = pd.MultiIndex.from_frame(seller_df[['run_id', 'seed']])\n",
    "            # Update seller_type for adversarial sellers in mimicry runs\n",
    "            seller_df.loc[seller_idx.isin(mimic_idx) & (seller_df['seller_type'] == 'adversary'), 'seller_type'] = 'adversary (mimic)'\n",
    "\n",
    "\n",
    "# --- 4. Main Plotting Logic ---\n",
    "if not seller_df.empty and Y_AXIS_COLUMN in seller_df.columns:\n",
    "    plt.figure(figsize=(12, 7))\n",
    "\n",
    "    palette = {\n",
    "        'benign': 'cornflowerblue',\n",
    "        'adversary': 'orangered',\n",
    "        'adversary (mimic)': 'darkviolet'\n",
    "    }\n",
    "\n",
    "    # Seaborn automatically calculates the mean and confidence interval (sd) across runs\n",
    "    sns.lineplot(\n",
    "        data=seller_df,\n",
    "        x='round',\n",
    "        y=Y_AXIS_COLUMN,\n",
    "        hue='seller_type',\n",
    "        style='seller_type',\n",
    "        palette=palette,\n",
    "        ci='sd'\n",
    "    )\n",
    "\n",
    "    # Final plot styling\n",
    "    plt.xlabel('Federated Round')\n",
    "    plt.ylabel(Y_AXIS_LABEL)\n",
    "    plt.title(PLOT_TITLE, pad=15)\n",
    "    plt.grid(True, which='major', linestyle='--', linewidth=0.5)\n",
    "    plt.legend(title='Seller Type')\n",
    "    # Use log scale if values vary widely, which is common for gradient norms\n",
    "    if Y_AXIS_COLUMN == 'gradient_norm':\n",
    "        plt.yscale('log')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    FIGURE_SAVE_DIR.mkdir(exist_ok=True)\n",
    "    save_path = FIGURE_SAVE_DIR / f\"{SAVE_FILENAME}.pdf\"\n",
    "    plt.savefig(save_path, bbox_inches='tight', format='pdf', dpi=300)\n",
    "    print(f\"✅ Figure saved to: {save_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(f\"❌ Plotting skipped. The seller DataFrame is empty or missing the '{Y_AXIS_COLUMN}' column.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CELL 10: Visualization 8 - Test Accuracy Learning Curve\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 1. Configuration for Visualization ---\n",
    "# TODO: Adjust these parameters to create your desired comparison.\n",
    "\n",
    "# The root directory containing your experiment runs.\n",
    "ROOT_EXPERIMENT_DIR = './path/to/your/experiment_outputs'\n",
    "\n",
    "# Optional filters to isolate specific runs for comparison.\n",
    "# For example, compare aggregation methods for a specific dataset.\n",
    "FILTERS = {\n",
    "    'experiment_dataset_name': 'cifar10',\n",
    "    # 'experiment_adv_rate': 0.3 # You can add more filters\n",
    "}\n",
    "\n",
    "# The column to use for separating lines on the plot (the comparison variable).\n",
    "HUE_COLUMN = 'aggregation_name'\n",
    "\n",
    "# --- Plot Customization ---\n",
    "PLOT_TITLE = \"Test Accuracy vs. Federated Rounds on CIFAR-10\"\n",
    "X_AXIS_LABEL = \"Federated Round\"\n",
    "Y_AXIS_LABEL = \"Test Accuracy\"\n",
    "SAVE_FILENAME = \"learning_curve_test_accuracy_cifar10\"\n",
    "FIGURE_SAVE_DIR = Path(\"./analysis_plots\")\n",
    "\n",
    "# --- 2. Data Loading and Merging ---\n",
    "# This uses functions from Cell 1, including the new one we added.\n",
    "try:\n",
    "    all_runs = find_successful_runs(ROOT_EXPERIMENT_DIR)\n",
    "\n",
    "    # Load the periodic evaluation results\n",
    "    eval_df = load_periodic_eval_df(all_runs)\n",
    "\n",
    "    # Load the summary data to get config parameters for filtering\n",
    "    summary_df = load_summary_df(all_runs)\n",
    "\n",
    "    # Merge config parameters into the evaluation data for easy filtering\n",
    "    if not eval_df.empty and not summary_df.empty:\n",
    "        config_cols = [col for col in summary_df.columns if col not in eval_df.columns] + ['run_id', 'seed']\n",
    "        plot_df = pd.merge(eval_df, summary_df[config_cols], on=['run_id', 'seed'])\n",
    "    else:\n",
    "        plot_df = pd.DataFrame()\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    plot_df = pd.DataFrame()\n",
    "\n",
    "# --- 3. Data Filtering ---\n",
    "if not plot_df.empty:\n",
    "    filtered_df = plot_df.copy()\n",
    "    for key, value in FILTERS.items():\n",
    "        if key in filtered_df.columns:\n",
    "            filtered_df = filtered_df[filtered_df[key] == value]\n",
    "        else:\n",
    "            print(f\"Warning: Filter key '{key}' not found. Skipping filter.\")\n",
    "\n",
    "# --- 4. Main Plotting Logic ---\n",
    "if not filtered_df.empty and 'test_accuracy' in filtered_df.columns:\n",
    "    plt.figure(figsize=(12, 7))\n",
    "\n",
    "    # Seaborn's lineplot automatically aggregates over the multiple seeds,\n",
    "    # plotting the mean as a line and the standard deviation as a shaded confidence band.\n",
    "    sns.lineplot(\n",
    "        data=filtered_df,\n",
    "        x='round',\n",
    "        y='test_accuracy',\n",
    "        hue=HUE_COLUMN,\n",
    "        style=HUE_COLUMN,\n",
    "        markers=True,\n",
    "        linewidth=2,\n",
    "        ci='sd' # 'ci=sd' shows the standard deviation\n",
    "    )\n",
    "\n",
    "    # --- Final Plot Styling ---\n",
    "    plt.xlabel(X_AXIS_LABEL)\n",
    "    plt.ylabel(Y_AXIS_LABEL)\n",
    "    plt.title(PLOT_TITLE, pad=15)\n",
    "    plt.grid(True, which='major', linestyle='--', linewidth=0.5)\n",
    "    plt.legend(title=HUE_COLUMN.replace('_', ' ').title())\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    FIGURE_SAVE_DIR.mkdir(exist_ok=True)\n",
    "    save_path = FIGURE_SAVE_DIR / f\"{SAVE_FILENAME}.pdf\"\n",
    "    plt.savefig(save_path, bbox_inches='tight', format='pdf', dpi=300)\n",
    "    print(f\"✅ Figure saved to: {save_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"❌ Plotting skipped. No data found after filtering or required columns are missing.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CELL 11: Visualization 9 - Impact of Data Heterogeneity (Non-IID)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. Configuration for Visualization ---\n",
    "# TODO: Adjust these parameters to create your desired comparison.\n",
    "\n",
    "# The root directory containing your experiment runs.\n",
    "ROOT_EXPERIMENT_DIR = './path/to/your/experiment_outputs'\n",
    "\n",
    "# Optional filters to isolate specific runs for comparison.\n",
    "# For example, analyze the impact of skew for a single aggregation method.\n",
    "FILTERS = {\n",
    "    'aggregation_name': 'FLAME',\n",
    "    'experiment_adv_rate': 0.3\n",
    "}\n",
    "\n",
    "# The performance metric you want to plot on the y-axis.\n",
    "Y_AXIS_COLUMN = 'test_accuracy'\n",
    "\n",
    "# --- Plot Customization ---\n",
    "PLOT_TITLE = f\"Impact of Data Heterogeneity on {FILTERS.get('aggregation_name', '')}\"\n",
    "X_AXIS_LABEL = \"Data Distribution (Dirichlet α)\"\n",
    "Y_AXIS_LABEL = \"Final Test Accuracy\"\n",
    "SAVE_FILENAME = f\"heterogeneity_impact_{FILTERS.get('aggregation_name', '')}\"\n",
    "FIGURE_SAVE_DIR = Path(\"./analysis_plots\")\n",
    "\n",
    "# --- 2. Data Loading ---\n",
    "# This uses the functions from Cell 1.\n",
    "try:\n",
    "    all_runs = find_successful_runs(ROOT_EXPERIMENT_DIR)\n",
    "    summary_df = load_summary_df(all_runs)\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    summary_df = pd.DataFrame()\n",
    "\n",
    "# --- 3. Data Preprocessing and Filtering ---\n",
    "if not summary_df.empty:\n",
    "    filtered_df = summary_df.copy()\n",
    "\n",
    "    # Apply all the filters defined in the configuration dictionary\n",
    "    for key, value in FILTERS.items():\n",
    "        if key in filtered_df.columns:\n",
    "            filtered_df = filtered_df[filtered_df[key] == value]\n",
    "        else:\n",
    "            print(f\"Warning: Filter key '{key}' not found. Skipping filter.\")\n",
    "\n",
    "    # The key column from your config that holds the Dirichlet alpha value\n",
    "    alpha_col = 'data_image_property_skew_dirichlet_alpha'\n",
    "\n",
    "    # Create descriptive labels for the x-axis\n",
    "    if alpha_col in filtered_df.columns:\n",
    "        # Sort by alpha value to ensure a logical order on the plot\n",
    "        filtered_df = filtered_df.sort_values(by=alpha_col, ascending=False)\n",
    "\n",
    "        # Create a more readable label for the plot\n",
    "        filtered_df['Distribution Label'] = filtered_df[alpha_col].apply(\n",
    "            lambda a: f\"Low Skew\\n(α={a})\" if a > 10 else (f\"Medium Skew\\n(α={a})\" if a == 1.0 else f\"High Skew\\n(α={a})\")\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Warning: Alpha column '{alpha_col}' not found. Cannot generate plot.\")\n",
    "        filtered_df = pd.DataFrame() # Invalidate df\n",
    "\n",
    "# --- 4. Main Plotting Logic ---\n",
    "if not filtered_df.empty and Y_AXIS_COLUMN in filtered_df.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Seaborn's barplot will automatically calculate the mean for each category\n",
    "    # and show the 95% confidence interval as error bars.\n",
    "    sns.barplot(\n",
    "        data=filtered_df,\n",
    "        x='Distribution Label',\n",
    "        y=Y_AXIS_COLUMN,\n",
    "        palette='viridis',\n",
    "        ci='sd' # Show standard deviation as error bars\n",
    "    )\n",
    "\n",
    "    # --- Final Plot Styling ---\n",
    "    plt.xlabel(X_AXIS_LABEL)\n",
    "    plt.ylabel(Y_AXIS_LABEL)\n",
    "    plt.title(PLOT_TITLE, pad=15)\n",
    "    plt.grid(True, which='major', linestyle='--', linewidth=0.5, axis='y')\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    FIGURE_SAVE_DIR.mkdir(exist_ok=True)\n",
    "    save_path = FIGURE_SAVE_DIR / f\"{SAVE_FILENAME}.pdf\"\n",
    "    plt.savefig(save_path, bbox_inches='tight', format='pdf', dpi=300)\n",
    "    print(f\"✅ Figure saved to: {save_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"❌ Plotting skipped. No data found after filtering or required columns are missing.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CELL 12: Visualization 11 - Buyer's Utility: Cost vs. Accuracy Frontier\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. Configuration for Visualization ---\n",
    "# TODO: Adjust these parameters to create your desired comparison.\n",
    "\n",
    "# The root directory containing your experiment runs.\n",
    "ROOT_EXPERIMENT_DIR = './path/to/your/experiment_outputs'\n",
    "\n",
    "# Define a specific scenario to ensure a fair comparison between methods.\n",
    "# For example, fix the dataset, model, and adversary rate.\n",
    "FILTERS = {\n",
    "    'experiment_dataset_name': 'cifar10',\n",
    "    'experiment_model_structure': 'resnet18',\n",
    "    'experiment_adv_rate': 0.3\n",
    "}\n",
    "\n",
    "# The column representing the different methods to compare.\n",
    "COMPARISON_COLUMN = 'aggregation_name'\n",
    "\n",
    "# --- Plot Customization ---\n",
    "PLOT_TITLE = f\"Buyer's Utility Frontier\\n(CIFAR-10, 30% Adversary Rate)\"\n",
    "X_AXIS_LABEL = \"Total Marketplace Cost (Resource Units)\"\n",
    "Y_AXIS_LABEL = \"Final Test Accuracy\"\n",
    "SAVE_FILENAME = \"buyer_utility_frontier_cifar10_adv03\"\n",
    "FIGURE_SAVE_DIR = Path(\"./analysis_plots\")\n",
    "\n",
    "# --- 2. Data Loading ---\n",
    "# This uses the functions from Cell 1.\n",
    "try:\n",
    "    all_runs = find_successful_runs(ROOT_EXPERIMENT_DIR)\n",
    "    summary_df = load_summary_df(all_runs)\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    summary_df = pd.DataFrame()\n",
    "\n",
    "# --- 3. Data Preprocessing and Filtering ---\n",
    "if not summary_df.empty:\n",
    "    # Standardize metric column names\n",
    "    summary_df = summary_df.rename(columns={\n",
    "        'avg_cost_per_round_benign': 'cost_benign',\n",
    "        'avg_cost_per_round_malicious': 'cost_malicious'\n",
    "    })\n",
    "\n",
    "    # Apply all the filters defined in the configuration dictionary\n",
    "    filtered_df = summary_df.copy()\n",
    "    for key, value in FILTERS.items():\n",
    "        if key in filtered_df.columns:\n",
    "            filtered_df = filtered_df[filtered_df[key] == value]\n",
    "        else:\n",
    "            print(f\"Warning: Filter key '{key}' not found. Skipping filter.\")\n",
    "\n",
    "    # ** Calculate Total Cost **\n",
    "    # This is a crucial step for this plot. We define total cost as the\n",
    "    # average cost per round multiplied by the number of rounds completed.\n",
    "    cost_cols = ['cost_benign', 'cost_malicious', 'completed_rounds']\n",
    "    if all(col in filtered_df.columns for col in cost_cols):\n",
    "        # Fill NaN costs with 0 before summing\n",
    "        filtered_df['total_cost'] = (filtered_df['cost_benign'].fillna(0) +\n",
    "                                     filtered_df['cost_malicious'].fillna(0)) * filtered_df['completed_rounds']\n",
    "    else:\n",
    "        print(\"Warning: Cost columns not found. Cannot calculate total_cost.\")\n",
    "        filtered_df['total_cost'] = np.nan\n",
    "\n",
    "# --- 4. Main Plotting Logic ---\n",
    "required_cols = ['test_accuracy', 'total_cost', COMPARISON_COLUMN]\n",
    "if not filtered_df.empty and all(col in filtered_df.columns for col in required_cols):\n",
    "\n",
    "    # --- Data Aggregation ---\n",
    "    # Group by the comparison column (e.g., aggregation_name) and find the\n",
    "    # average performance and cost across all random seeds for that method.\n",
    "    plot_data = filtered_df.groupby(COMPARISON_COLUMN)[['test_accuracy', 'total_cost']].mean().reset_index()\n",
    "\n",
    "    # --- Plotting Scatter Plot ---\n",
    "    plt.figure(figsize=(10, 7))\n",
    "\n",
    "    sns.scatterplot(\n",
    "        data=plot_data,\n",
    "        x='total_cost',\n",
    "        y='test_accuracy',\n",
    "        hue=COMPARISON_COLUMN,\n",
    "        s=200, # Make markers larger\n",
    "        style=COMPARISON_COLUMN,\n",
    "        markers=True,\n",
    "        edgecolor='black',\n",
    "        legend='full'\n",
    "    )\n",
    "\n",
    "    # Annotate each point with its name for clarity\n",
    "    for i, point in plot_data.iterrows():\n",
    "        plt.text(point['total_cost'] * 1.01, point['test_accuracy'], point[COMPARISON_COLUMN],\n",
    "                 horizontalalignment='left', size='small', color='black')\n",
    "\n",
    "    # --- Final Plot Styling ---\n",
    "    plt.xlabel(X_AXIS_LABEL)\n",
    "    plt.ylabel(Y_AXIS_LABEL)\n",
    "    plt.title(PLOT_TITLE, pad=15)\n",
    "    plt.grid(True, which='major', linestyle='--', linewidth=0.5)\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.xlim(left=0)\n",
    "\n",
    "    # Add an arrow and text to indicate the \"ideal\" direction\n",
    "    plt.annotate('More Efficient →', xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "                 xytext=(0.25, 0.85), textcoords='axes fraction',\n",
    "                 arrowprops=dict(facecolor='green', shrink=0.05),\n",
    "                 fontsize=12, color='green')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    FIGURE_SAVE_DIR.mkdir(exist_ok=True)\n",
    "    save_path = FIGURE_SAVE_DIR / f\"{SAVE_FILENAME}.pdf\"\n",
    "    plt.savefig(save_path, bbox_inches='tight', format='pdf', dpi=300)\n",
    "    print(f\"✅ Figure saved to: {save_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"❌ Plotting skipped. No data found after filtering or required columns are missing.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CELL 13: Visualization 12 - Seller's Utility: Contribution vs. Reward\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. Configuration for Visualization ---\n",
    "# TODO: Adjust these parameters to create your desired comparison.\n",
    "\n",
    "# The root directory containing your experiment runs.\n",
    "ROOT_EXPERIMENT_DIR = './path/to/your/experiment_outputs'\n",
    "\n",
    "# Define a specific scenario to analyze.\n",
    "# IMPORTANT: Choose an aggregation method that HAS an incentive mechanism (e.g., FLAME, MartFL).\n",
    "FILTERS = {\n",
    "    'aggregation_name': 'FLAME',\n",
    "    'experiment_dataset_name': 'cifar10',\n",
    "    'experiment_adv_rate': 0.3\n",
    "}\n",
    "\n",
    "# Select a single, representative round to get a snapshot of the marketplace dynamics.\n",
    "# A mid-training round is often a good choice.\n",
    "REPRESENTATIVE_ROUND = 50\n",
    "\n",
    "# --- Plot Customization ---\n",
    "PLOT_TITLE = f\"Seller Utility: Contribution vs. Reward\\n({FILTERS.get('aggregation_name', '')}, Round {REPRESENTATIVE_ROUND})\"\n",
    "X_AXIS_LABEL = \"Seller Contribution (Gradient L2 Norm)\"\n",
    "Y_AXIS_LABEL = \"Seller Reward (Aggregation Weight)\"\n",
    "SAVE_FILENAME = f\"seller_utility_{FILTERS.get('aggregation_name', '')}_round{REPRESENTATIVE_ROUND}\"\n",
    "FIGURE_SAVE_DIR = Path(\"./analysis_plots\")\n",
    "\n",
    "# --- 2. Data Loading ---\n",
    "# This uses the functions from Cell 1.\n",
    "try:\n",
    "    all_runs = find_successful_runs(ROOT_EXPERIMENT_DIR)\n",
    "    seller_df = load_seller_analysis_df(all_runs)\n",
    "    summary_df = load_summary_df(all_runs) # Needed for config params\n",
    "\n",
    "    # Merge config parameters into the seller data for easy filtering\n",
    "    if not seller_df.empty and not summary_df.empty:\n",
    "        config_cols = [col for col in summary_df.columns if col not in seller_df.columns] + ['run_id', 'seed']\n",
    "        plot_df = pd.merge(seller_df, summary_df[config_cols], on=['run_id', 'seed'], how='left')\n",
    "    else:\n",
    "        plot_df = pd.DataFrame()\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    plot_df = pd.DataFrame()\n",
    "\n",
    "# --- 3. Data Preprocessing and Filtering ---\n",
    "if not plot_df.empty:\n",
    "    filtered_df = plot_df.copy()\n",
    "\n",
    "    # Apply all the filters defined in the configuration dictionary\n",
    "    for key, value in FILTERS.items():\n",
    "        if key in filtered_df.columns:\n",
    "            filtered_df = filtered_df[filtered_df[key] == value]\n",
    "        else:\n",
    "            print(f\"Warning: Filter key '{key}' not found. Skipping filter.\")\n",
    "\n",
    "    # ** Isolate the specific data for this plot **\n",
    "    # 1. Filter for the single representative round.\n",
    "    # 2. Filter for ONLY benign sellers.\n",
    "    snapshot_df = filtered_df[\n",
    "        (filtered_df['round'] == REPRESENTATIVE_ROUND) &\n",
    "        (filtered_df['seller_type'] == 'benign')\n",
    "    ].copy()\n",
    "\n",
    "# --- 4. Main Plotting Logic ---\n",
    "required_cols = ['gradient_norm', 'weight']\n",
    "if not snapshot_df.empty and all(col in snapshot_df.columns for col in required_cols):\n",
    "\n",
    "    # --- Plotting Scatter Plot with Regression Line ---\n",
    "    plt.figure(figsize=(10, 7))\n",
    "\n",
    "    # regplot combines a scatter plot with a linear regression model fit\n",
    "    # It automatically calculates and displays the best-fit line and its confidence interval.\n",
    "    sns.regplot(\n",
    "        data=snapshot_df,\n",
    "        x='gradient_norm',\n",
    "        y='weight',\n",
    "        scatter_kws={'alpha': 0.6, 's': 50}, # Style the points\n",
    "        line_kws={'color': 'crimson', 'linewidth': 2} # Style the line\n",
    "    )\n",
    "\n",
    "    # Calculate the Pearson correlation coefficient to quantify the relationship\n",
    "    correlation = snapshot_df['gradient_norm'].corr(snapshot_df['weight'])\n",
    "\n",
    "    # --- Final Plot Styling ---\n",
    "    plt.xlabel(X_AXIS_LABEL)\n",
    "    plt.ylabel(Y_AXIS_LABEL)\n",
    "    plt.title(PLOT_TITLE, pad=15)\n",
    "    plt.grid(True, which='major', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    # Add an annotation with the correlation value\n",
    "    plt.annotate(f\"Pearson's r = {correlation:.3f}\",\n",
    "                 xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "                 fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"wheat\", alpha=0.5))\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    FIGURE_SAVE_DIR.mkdir(exist_ok=True)\n",
    "    save_path = FIGURE_SAVE_DIR / f\"{SAVE_FILENAME}.pdf\"\n",
    "    plt.savefig(save_path, bbox_inches='tight', format='pdf', dpi=300)\n",
    "    print(f\"✅ Figure saved to: {save_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"❌ Plotting skipped. No data found after filtering or required columns are missing.\")\n",
    "    if 'snapshot_df' in locals() and snapshot_df.empty:\n",
    "        print(f\"   Note: No benign seller data found for round {REPRESENTATIVE_ROUND} with the specified filters.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
